2023-10-20 18:32:27,915	INFO worker.py:1458 -- Connecting to existing Ray cluster at address: 192.0.0.1:6379...
2023-10-20 18:32:27,930	INFO worker.py:1642 -- Connected to Ray cluster.
2023-10-20 18:32:35,739	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/nevergrad/parametrization/_datalayers.py:107: NevergradRuntimeWarning: Bounds are 2.0000000000000004 sigma away from each other at the closest, you should aim for at least 3 for better quality.
  warnings.warn(
2023-10-20 18:32:38,683	WARNING tune.py:997 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[2m[36m(TrainTrainable pid=365342)[0m Trainable.setup took 15.933 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['365773 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=365773)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TrainTrainable pid=365772)[0m Trainable.setup took 14.070 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=365773)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=365773)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=365773)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=365773)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=365773)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/lightning_logs
[2m[36m(RayTrainWorker pid=365773)[0m 
[2m[36m(RayTrainWorker pid=365773)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=365773)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=365773)[0m 0 | s1   | Sequential | 59.3 K
[2m[36m(RayTrainWorker pid=365773)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=365773)[0m 59.3 K    Trainable params
[2m[36m(RayTrainWorker pid=365773)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=365773)[0m 59.3 K    Total params
[2m[36m(RayTrainWorker pid=365773)[0m 0.237     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=365773)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=365773)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=365773)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=365773)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=365773)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=365773)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000000)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000001)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000002)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000003)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000004)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000005)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000006)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000007)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000008)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000009)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000010)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000011)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000012)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000013)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000014)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000015)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000016)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000017)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000018)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000019)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000020)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000021)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000022)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000023)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000024)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000025)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000026)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000027)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000028)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000029)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000030)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000031)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000032)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000033)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000034)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000035)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000036)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000037)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000038)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000039)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000040)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000041)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000042)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000043)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000044)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000045)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000046)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000047)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000048)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000049)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000050)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000051)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000052)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000053)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000054)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000055)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000056)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000057)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000058)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000059)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000060)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000061)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000062)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000063)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000064)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000065)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000066)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000067)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000068)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000069)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000070)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000071)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000072)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000073)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000074)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000075)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000076)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000077)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000078)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000079)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000080)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000081)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000082)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000083)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000084)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000085)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000086)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000087)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000088)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000089)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000090)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000091)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000092)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000093)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000094)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000095)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000096)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000097)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000098)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000099)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000100)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000101)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000102)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000103)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000104)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000105)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000106)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000107)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000108)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000109)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000110)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000111)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000112)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000113)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000114)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000115)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000116)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000117)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000118)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000119)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000120)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000121)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000122)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000123)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000124)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000125)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000126)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000127)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000128)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000129)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000130)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000131)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000132)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000133)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000134)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000135)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000136)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000137)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000138)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000139)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000140)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000141)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000142)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000143)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000144)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000145)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000146)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000147)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000148)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000149)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000150)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000151)
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['366456 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000152)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000153)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000154)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000155)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000156)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000157)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000158)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000159)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000160)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000161)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000162)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000163)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000164)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000165)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000166)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000167)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000168)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000169)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000170)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000171)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000172)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000173)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000174)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000175)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000176)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000177)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000178)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000179)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000180)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000181)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000182)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000183)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000184)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000185)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000186)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000187)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000188)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000189)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000190)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000191)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000192)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000193)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000194)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000195)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000196)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000197)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000198)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000199)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000200)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000201)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000202)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000203)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000204)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000205)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000206)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000207)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000208)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000209)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000210)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000211)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000212)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000213)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000214)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000215)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000216)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000217)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000218)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000219)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000220)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000221)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000222)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000223)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000224)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000225)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000226)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000227)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000228)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000229)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000230)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000231)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000232)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000233)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000234)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000235)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000236)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000237)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000238)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000239)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000240)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000241)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000242)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000243)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000244)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000245)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000246)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000247)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000248)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000249)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000250)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000251)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000252)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000253)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000254)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000255)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000256)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000257)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000258)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000259)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000260)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000261)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000262)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000263)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000264)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000265)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000266)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000267)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000268)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000269)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000270)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000271)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000272)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000273)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000274)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000275)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000276)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000277)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000278)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000279)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000280)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000281)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000282)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000283)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000284)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000285)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000286)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000287)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000288)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000289)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000290)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000291)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000292)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000293)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000294)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000295)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000296)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000297)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000298)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000299)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000300)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000301)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000302)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000303)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000304)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000305)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000306)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000307)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000308)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000309)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000310)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000311)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000312)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000313)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000314)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000315)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000316)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000317)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000318)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000319)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000320)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000321)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000322)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000323)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000324)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000325)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000326)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000327)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000328)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000329)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000330)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000331)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000332)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000333)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000334)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000335)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000336)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000337)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000338)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000339)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000340)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000341)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000342)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000343)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000344)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000345)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000346)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000347)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000348)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000349)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000350)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000351)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000352)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000353)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000354)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000355)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000356)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000357)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000358)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000359)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000360)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000361)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000362)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000363)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000364)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000365)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000366)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000367)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000368)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000369)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000370)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000371)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000372)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000373)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000374)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000375)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000376)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000377)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000378)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000379)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000380)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000381)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000382)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000383)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000384)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000385)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000386)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000387)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000388)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000389)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000390)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000391)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000392)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000393)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000394)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000395)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000396)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000397)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000398)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000399)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000400)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000401)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000402)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000403)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000404)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000405)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000406)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000407)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000408)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000409)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000410)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000411)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000412)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000413)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000414)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000415)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000416)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000417)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000418)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000419)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000420)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000421)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000422)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000423)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000424)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000425)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000426)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000427)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000428)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000429)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000430)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000431)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000432)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000433)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000434)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000435)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000436)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000437)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000438)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000439)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000440)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000441)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000442)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000443)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000444)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000445)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000446)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000447)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000448)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000449)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000450)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000451)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000452)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000453)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000454)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000455)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000456)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000457)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000458)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000459)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000460)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000461)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000462)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000463)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000464)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000465)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000466)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000467)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000468)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000469)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000470)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000471)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000472)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000473)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000474)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000475)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000476)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000477)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000478)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000479)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000480)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000481)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000482)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000483)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000484)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000485)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000486)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000487)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000488)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000489)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000490)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000491)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000492)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000493)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000494)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000495)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000496)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000497)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000498)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000499)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000500)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000501)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000502)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000503)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000504)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000505)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000506)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000507)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000508)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000509)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000510)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000511)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000512)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000513)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000514)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000515)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000516)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000517)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000518)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000519)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000520)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000521)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000522)
[2m[36m(RayTrainWorker pid=366456)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000523)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000524)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000525)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000526)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000527)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000528)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000529)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000530)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000531)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000532)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000533)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000534)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000535)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000536)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000537)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000538)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000539)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000540)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000541)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000542)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000543)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000544)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000545)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000546)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000547)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000548)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000549)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000550)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000551)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000552)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000553)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000554)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000555)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000556)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000557)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000558)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000559)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000560)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000561)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000562)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000563)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000564)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000565)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000566)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000567)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000568)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000569)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000570)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000571)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000572)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000573)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000574)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000575)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000576)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000577)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000578)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000579)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000580)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000581)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000582)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000583)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000584)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000585)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000586)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000587)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000588)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000589)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000590)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000591)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000592)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000593)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000594)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000595)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000596)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000597)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000598)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000599)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000600)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000601)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000602)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000603)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000604)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000605)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000606)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000607)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000608)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000609)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000610)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000611)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000612)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000613)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000614)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000615)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000616)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000617)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000618)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000619)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000620)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000621)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000622)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000623)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000624)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000625)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000626)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000627)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000628)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000629)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000630)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000631)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000632)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000633)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000634)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000635)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000636)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000637)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000638)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000639)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000640)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000641)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000642)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000643)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000644)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000645)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000646)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000647)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000648)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000649)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000650)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000651)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000652)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000653)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000654)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000655)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000656)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000657)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000658)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000659)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000660)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000661)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000662)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000663)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000664)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000665)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000666)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000667)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000668)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000669)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000670)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000671)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000672)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000673)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000674)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000675)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000676)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000677)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000678)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000679)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000680)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000681)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000682)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000683)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000684)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000685)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000686)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000687)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000688)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000689)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000690)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000691)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000692)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000693)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000694)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000695)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000696)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000697)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000698)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000699)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000700)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000701)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000702)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000703)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000704)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000705)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000706)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000707)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000708)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000709)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000710)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000711)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000712)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000713)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000714)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000715)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000716)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000717)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000718)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000719)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000720)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000721)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000722)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000723)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000724)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000725)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000726)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000727)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000728)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000729)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000730)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000731)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000732)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000733)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000734)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000735)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000736)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000737)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000738)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000739)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000740)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000741)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000742)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000743)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000744)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000745)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000746)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000747)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000748)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000749)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000750)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000751)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000752)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000753)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000754)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000755)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000756)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000757)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000758)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000759)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000760)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000761)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000762)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000763)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000764)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000765)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000766)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000767)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000768)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000769)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000770)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000771)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000772)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000773)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000774)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000775)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000776)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000777)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000778)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000779)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000780)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000781)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000782)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000783)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000784)
[2m[36m(TrainTrainable pid=366455)[0m Trainable.setup took 10.707 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000785)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000786)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000787)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000788)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000789)
[2m[36m(RayTrainWorker pid=366456)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=366456)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=366456)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=366456)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000790)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000791)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000792)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000793)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000794)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000795)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000796)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000797)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000798)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000799)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000800)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000801)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000802)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000803)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000804)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000805)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000806)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000807)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000808)
[2m[36m(RayTrainWorker pid=366456)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cf78dd1e_2_batch_size=32,layer_1_size=46,layer_2_size=143,layer_3_size=157,layer_4_size=206,layer_5_size=56,layer_6_s_2023-10-20_18-33-31/lightning_logs
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000809)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000810)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000811)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000812)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000813)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000814)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000815)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000816)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000817)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000818)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000819)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000820)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000821)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000822)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000823)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000824)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000825)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000826)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000827)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000828)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000829)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000830)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000831)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000832)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000833)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000834)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000835)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000836)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000837)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000838)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000839)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000840)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000841)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000842)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000843)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000844)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000845)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000846)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000847)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000848)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000849)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000850)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000851)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000852)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000853)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000854)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000855)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000856)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000857)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000858)
[2m[36m(RayTrainWorker pid=366456)[0m 
[2m[36m(RayTrainWorker pid=366456)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=366456)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=366456)[0m 0 | s1   | Sequential | 83.9 K
[2m[36m(RayTrainWorker pid=366456)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=366456)[0m 83.9 K    Trainable params
[2m[36m(RayTrainWorker pid=366456)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=366456)[0m 83.9 K    Total params
[2m[36m(RayTrainWorker pid=366456)[0m 0.336     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=366456)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=366456)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=366456)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=366456)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=366456)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=366456)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000859)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000860)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000861)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000862)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000863)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000864)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000865)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000866)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000867)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000868)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000869)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000870)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000871)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000872)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000873)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000874)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000875)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000876)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000877)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000878)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000879)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000880)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000881)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000882)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000883)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000884)
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_000993)[32m [repeated 208x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['367089 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001109)[32m [repeated 228x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001155)[32m [repeated 157x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001196)[32m [repeated 149x across cluster][0m
[2m[36m(RayTrainWorker pid=367089)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001235)[32m [repeated 144x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001278)[32m [repeated 151x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001320)[32m [repeated 157x across cluster][0m
[2m[36m(TrainTrainable pid=367088)[0m Trainable.setup took 12.715 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=367089)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=367089)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=367089)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=367089)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=367089)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_519a50a1_3_batch_size=128,layer_1_size=58,layer_2_size=209,layer_3_size=126,layer_4_size=180,layer_5_size=202,layer_6_2023-10-20_18-34-20/lightning_logs
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001358)[32m [repeated 81x across cluster][0m
[2m[36m(RayTrainWorker pid=367089)[0m 
[2m[36m(RayTrainWorker pid=367089)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=367089)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=367089)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=367089)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=367089)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=367089)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=367089)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=367089)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=367089)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=367089)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=367089)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=367089)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=367089)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=367089)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001396)[32m [repeated 72x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001435)[32m [repeated 106x across cluster][0m
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['367655 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001478)[32m [repeated 43x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['367889 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001520)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001570)[32m [repeated 50x across cluster][0m
[2m[36m(RayTrainWorker pid=367655)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001617)[32m [repeated 47x across cluster][0m
[2m[36m(RayTrainWorker pid=367889)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001654)[32m [repeated 37x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001691)[32m [repeated 37x across cluster][0m
[2m[36m(TrainTrainable pid=367888)[0m Trainable.setup took 12.847 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=367889)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=367889)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=367889)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=367889)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=367889)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_01e3ddd3_5_batch_size=128,layer_1_size=197,layer_2_size=164,layer_3_size=80,layer_4_size=226,layer_5_size=177,layer_6_2023-10-20_18-35-45/lightning_logs
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001789)[32m [repeated 98x across cluster][0m
[2m[36m(RayTrainWorker pid=367655)[0m 
[2m[36m(RayTrainWorker pid=367655)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=367655)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=367655)[0m 0 | s1   | Sequential | 83.9 K
[2m[36m(RayTrainWorker pid=367655)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=367655)[0m 83.9 K    Trainable params
[2m[36m(RayTrainWorker pid=367655)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=367655)[0m 83.9 K    Total params
[2m[36m(RayTrainWorker pid=367655)[0m 0.336     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=367655)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=367655)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=367655)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=367655)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(TrainTrainable pid=367656)[0m Trainable.setup took 12.848 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=367655)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=367655)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=367655)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=367655)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=367655)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2df67ee0_4_batch_size=128,layer_1_size=131,layer_2_size=34,layer_3_size=57,layer_4_size=117,layer_5_size=108,layer_6__2023-10-20_18-35-03/lightning_logs
[2m[36m(RayTrainWorker pid=367655)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2df67ee0_4_batch_size=128,layer_1_size=131,layer_2_size=34,layer_3_size=57,layer_4_size=117,layer_5_size=108,layer_6__2023-10-20_18-35-03/checkpoint_000015)[32m [repeated 112x across cluster][0m
[2m[36m(RayTrainWorker pid=367889)[0m 
[2m[36m(RayTrainWorker pid=367889)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=367889)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=367889)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=367889)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=367889)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=367889)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=367889)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=367889)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=367889)[0m   rank_zero_warn([32m [repeated 5x across cluster][0m
[2m[36m(RayTrainWorker pid=367889)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=367889)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=367655)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2df67ee0_4_batch_size=128,layer_1_size=131,layer_2_size=34,layer_3_size=57,layer_4_size=117,layer_5_size=108,layer_6__2023-10-20_18-35-03/checkpoint_000070)[32m [repeated 119x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['368905 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_001958)[32m [repeated 140x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002077)[32m [repeated 119x across cluster][0m
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['369346 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002158)[32m [repeated 81x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['369351 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002210)[32m [repeated 52x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002261)[32m [repeated 51x across cluster][0m
[2m[36m(RayTrainWorker pid=368905)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002311)[32m [repeated 50x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002361)[32m [repeated 50x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002410)[32m [repeated 49x across cluster][0m
[2m[36m(TrainTrainable pid=368873)[0m Trainable.setup took 15.424 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=368905)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=368905)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=368905)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=368905)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=368905)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ee53bc42_6_batch_size=64,layer_1_size=189,layer_2_size=176,layer_3_size=108,layer_4_size=232,layer_5_size=100,layer_6_2023-10-20_18-35-57/lightning_logs
[2m[36m(RayTrainWorker pid=368905)[0m 
[2m[36m(RayTrainWorker pid=368905)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=368905)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=368905)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=368905)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=368905)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=368905)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=368905)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=368905)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=368905)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=368905)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=368905)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=368905)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=368905)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=368905)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002456)[32m [repeated 46x across cluster][0m
[2m[36m(TrainTrainable pid=369347)[0m Trainable.setup took 15.425 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m GPU available: False, used: False[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m TPU available: False, using: 0 TPU cores[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m IPU available: False, using: 0 IPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m HPU available: False, using: 0 HPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17550d9f_7_batch_size=32,layer_1_size=177,layer_2_size=87,layer_3_size=46,layer_4_size=75,layer_5_size=53,layer_6_siz_2023-10-20_18-36-31/lightning_logs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m   | Name | Type       | Params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m ------------------------------------[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m 0 | s1   | Sequential | 83.9 K[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m 83.9 K    Trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m 0         Non-trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m 83.9 K    Total params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m 0.336     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m   rank_zero_warn([32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002486)[32m [repeated 96x across cluster][0m
[2m[36m(RayTrainWorker pid=369351)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9ec46a23_8_batch_size=32,layer_1_size=227,layer_2_size=86,layer_3_size=193,layer_4_size=230,layer_5_size=146,layer_6__2023-10-20_18-36-47/checkpoint_000055)[32m [repeated 113x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002530)[32m [repeated 122x across cluster][0m
[2m[36m(TorchTrainer pid=369350)[0m Starting distributed worker processes: ['370593 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=368905)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ee53bc42_6_batch_size=64,layer_1_size=189,layer_2_size=176,layer_3_size=108,layer_4_size=232,layer_5_size=100,layer_6_2023-10-20_18-35-57/checkpoint_000156)[32m [repeated 101x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['371125 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=368905)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ee53bc42_6_batch_size=64,layer_1_size=189,layer_2_size=176,layer_3_size=108,layer_4_size=232,layer_5_size=100,layer_6_2023-10-20_18-35-57/checkpoint_000200)[32m [repeated 122x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002627)[32m [repeated 119x across cluster][0m
[2m[36m(RayTrainWorker pid=370593)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002703)[32m [repeated 151x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['371530 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=371125)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002779)[32m [repeated 160x across cluster][0m
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002893)[32m [repeated 217x across cluster][0m
[2m[36m(RayTrainWorker pid=371530)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TrainTrainable pid=370594)[0m Trainable.setup took 14.342 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=370593)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=370593)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=370593)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=370593)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_002988)[32m [repeated 188x across cluster][0m
[2m[36m(RayTrainWorker pid=371125)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_78ae6442_10_batch_size=32,layer_1_size=123,layer_2_size=137,layer_3_size=201,layer_4_size=104,layer_5_size=191,layer__2023-10-20_18-37-25/lightning_logs
[2m[36m(RayTrainWorker pid=370593)[0m 
[2m[36m(RayTrainWorker pid=370593)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=370593)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=370593)[0m 0 | s1   | Sequential | 76.4 K
[2m[36m(RayTrainWorker pid=370593)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=370593)[0m 76.4 K    Trainable params
[2m[36m(RayTrainWorker pid=370593)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=370593)[0m 76.4 K    Total params
[2m[36m(RayTrainWorker pid=370593)[0m 0.305     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=370593)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=370593)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=371125)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=371125)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(TrainTrainable pid=371126)[0m Trainable.setup took 14.357 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=371125)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=371125)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=371125)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=371125)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=365773)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_59331a63_1_batch_size=32,layer_1_size=208,layer_2_size=59,layer_3_size=120,layer_4_size=139,layer_5_size=82,layer_6_s_2023-10-20_18-32-38/checkpoint_003092)[32m [repeated 195x across cluster][0m
[2m[36m(RayTrainWorker pid=370593)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d9a54cb1_9_batch_size=32,layer_1_size=64,layer_2_size=84,layer_3_size=169,layer_4_size=153,layer_5_size=37,layer_6_si_2023-10-20_18-36-47/lightning_logs
[2m[36m(RayTrainWorker pid=371530)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=371530)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=371530)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=371530)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=371530)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f8a26b5e_11_batch_size=32,layer_1_size=49,layer_2_size=246,layer_3_size=231,layer_4_size=72,layer_5_size=157,layer_6__2023-10-20_18-37-44/lightning_logs
[2m[36m(RayTrainWorker pid=371125)[0m 
[2m[36m(RayTrainWorker pid=371125)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=371125)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=371125)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=371125)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=371125)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=371125)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=371125)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=371125)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=370593)[0m   rank_zero_warn([32m [repeated 5x across cluster][0m
[2m[36m(RayTrainWorker pid=370593)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=370593)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=371530)[0m 
[2m[36m(RayTrainWorker pid=371530)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=371530)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=371530)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=371530)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=371530)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=371530)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=371530)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=371530)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=371530)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=369346)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17550d9f_7_batch_size=32,layer_1_size=177,layer_2_size=87,layer_3_size=46,layer_4_size=75,layer_5_size=53,layer_6_siz_2023-10-20_18-36-31/checkpoint_000724)[32m [repeated 191x across cluster][0m
[2m[36m(RayTrainWorker pid=371530)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=371530)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17550d9f_7_batch_size=32,layer_1_size=177,layer_2_size=87,layer_3_size=46,layer_4_size=75,layer_5_size=53,layer_6_siz_2023-10-20_18-36-31/checkpoint_000757)[32m [repeated 151x across cluster][0m
[2m[36m(RayTrainWorker pid=369346)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17550d9f_7_batch_size=32,layer_1_size=177,layer_2_size=87,layer_3_size=46,layer_4_size=75,layer_5_size=53,layer_6_siz_2023-10-20_18-36-31/checkpoint_000773)[32m [repeated 89x across cluster][0m
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['372281 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=369346)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17550d9f_7_batch_size=32,layer_1_size=177,layer_2_size=87,layer_3_size=46,layer_4_size=75,layer_5_size=53,layer_6_siz_2023-10-20_18-36-31/checkpoint_000800)[32m [repeated 96x across cluster][0m
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['373699 (192.0.0.1)'][32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=371530)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f8a26b5e_11_batch_size=32,layer_1_size=49,layer_2_size=246,layer_3_size=231,layer_4_size=72,layer_5_size=157,layer_6__2023-10-20_18-37-44/checkpoint_000235)[32m [repeated 97x across cluster][0m
[2m[36m(RayTrainWorker pid=371530)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f8a26b5e_11_batch_size=32,layer_1_size=49,layer_2_size=246,layer_3_size=231,layer_4_size=72,layer_5_size=157,layer_6__2023-10-20_18-37-44/checkpoint_000298)[32m [repeated 63x across cluster][0m
[2m[36m(RayTrainWorker pid=371530)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f8a26b5e_11_batch_size=32,layer_1_size=49,layer_2_size=246,layer_3_size=231,layer_4_size=72,layer_5_size=157,layer_6__2023-10-20_18-37-44/checkpoint_000355)[32m [repeated 57x across cluster][0m
[2m[36m(RayTrainWorker pid=372281)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TrainTrainable pid=372277)[0m Trainable.setup took 14.495 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=371530)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f8a26b5e_11_batch_size=32,layer_1_size=49,layer_2_size=246,layer_3_size=231,layer_4_size=72,layer_5_size=157,layer_6__2023-10-20_18-37-44/checkpoint_000400)[32m [repeated 45x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=372281)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=372281)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=372281)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=372281)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=372281)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0339a4a2_13_batch_size=256,layer_1_size=160,layer_2_size=58,layer_3_size=211,layer_4_size=166,layer_5_size=96,layer_6_2023-10-20_18-38-27/lightning_logs
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['374759 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=372281)[0m 
[2m[36m(RayTrainWorker pid=372281)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=372281)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=372281)[0m 0 | s1   | Sequential | 59.3 K
[2m[36m(RayTrainWorker pid=372281)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=372281)[0m 59.3 K    Trainable params
[2m[36m(RayTrainWorker pid=372281)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=372281)[0m 59.3 K    Total params
[2m[36m(RayTrainWorker pid=372281)[0m 0.237     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=372281)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=372281)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=372281)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=372281)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=372281)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=372281)[0m   rank_zero_warn(
[2m[36m(TrainTrainable pid=373700)[0m Trainable.setup took 14.463 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=372281)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0339a4a2_13_batch_size=256,layer_1_size=160,layer_2_size=58,layer_3_size=211,layer_4_size=166,layer_5_size=96,layer_6_2023-10-20_18-38-27/checkpoint_000001)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m GPU available: False, used: False[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m TPU available: False, using: 0 TPU cores[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m IPU available: False, using: 0 IPUs[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m HPU available: False, using: 0 HPUs[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_a83c46b2_16_batch_size=128,layer_1_size=153,layer_2_size=202,layer_3_size=124,layer_4_size=143,layer_5_size=216,layer_2023-10-20_18-38-44/lightning_logs[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m [32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m   | Name | Type       | Params[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m ------------------------------------[32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m 0 | s1   | Sequential | 83.9 K[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m 83.9 K    Trainable params[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m 0         Non-trainable params[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m 83.9 K    Total params[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m 0.336     Total estimated model params size (MB)[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m   rank_zero_warn([32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=373699)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=372276)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_35c584cc_12_batch_size=256,layer_1_size=215,layer_2_size=130,layer_3_size=90,layer_4_size=225,layer_5_size=37,layer_6_2023-10-20_18-37-54/checkpoint_000039)[32m [repeated 187x across cluster][0m
[2m[36m(RayTrainWorker pid=372281)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0339a4a2_13_batch_size=256,layer_1_size=160,layer_2_size=58,layer_3_size=211,layer_4_size=166,layer_5_size=96,layer_6_2023-10-20_18-38-27/checkpoint_000069)[32m [repeated 184x across cluster][0m
[2m[36m(RayTrainWorker pid=374759)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=372281)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0339a4a2_13_batch_size=256,layer_1_size=160,layer_2_size=58,layer_3_size=211,layer_4_size=166,layer_5_size=96,layer_6_2023-10-20_18-38-27/checkpoint_000115)[32m [repeated 174x across cluster][0m
[2m[36m(RayTrainWorker pid=372281)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0339a4a2_13_batch_size=256,layer_1_size=160,layer_2_size=58,layer_3_size=211,layer_4_size=166,layer_5_size=96,layer_6_2023-10-20_18-38-27/checkpoint_000161)[32m [repeated 150x across cluster][0m
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000205)[32m [repeated 112x across cluster][0m
[2m[36m(TrainTrainable pid=374758)[0m Trainable.setup took 15.456 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=374759)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=374759)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=374759)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=374759)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=374759)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_6bb23030_17_batch_size=128,layer_1_size=73,layer_2_size=216,layer_3_size=66,layer_4_size=143,layer_5_size=182,layer_6_2023-10-20_18-38-45/lightning_logs
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000251)[32m [repeated 46x across cluster][0m
[2m[36m(RayTrainWorker pid=374759)[0m 
[2m[36m(RayTrainWorker pid=374759)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=374759)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=374759)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=374759)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=374759)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=374759)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=374759)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=374759)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=374759)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=374759)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=374759)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=374759)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=374759)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=374759)[0m   rank_zero_warn(
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['375992 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=374759)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_6bb23030_17_batch_size=128,layer_1_size=73,layer_2_size=216,layer_3_size=66,layer_4_size=143,layer_5_size=182,layer_6_2023-10-20_18-38-45/checkpoint_000017)[32m [repeated 35x across cluster][0m
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['375748 (192.0.0.1)'][32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000308)[32m [repeated 78x across cluster][0m
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000367)[32m [repeated 163x across cluster][0m
[2m[36m(RayTrainWorker pid=375752)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000443)[32m [repeated 117x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000551)[32m [repeated 108x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['377478 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000643)[32m [repeated 92x across cluster][0m
[2m[36m(TrainTrainable pid=375749)[0m Trainable.setup took 12.535 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=375752)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=375752)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=375752)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=375752)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=375752)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_631029cc_19_batch_size=32,layer_1_size=49,layer_2_size=179,layer_3_size=135,layer_4_size=48,layer_5_size=64,layer_6_s_2023-10-20_18-39-36/lightning_logs
[2m[36m(RayTrainWorker pid=375752)[0m 
[2m[36m(RayTrainWorker pid=375752)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=375752)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=375752)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=375752)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=375752)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=375752)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=375752)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=375752)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=375752)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=375752)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=375752)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=375752)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=375752)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=375752)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000689)[32m [repeated 46x across cluster][0m
[2m[36m(TrainTrainable pid=375993)[0m Trainable.setup took 12.533 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m GPU available: False, used: False[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m TPU available: False, using: 0 TPU cores[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m IPU available: False, using: 0 IPUs[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m HPU available: False, using: 0 HPUs[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b7401655_21_batch_size=128,layer_1_size=74,layer_2_size=240,layer_3_size=237,layer_4_size=104,layer_5_size=199,layer__2023-10-20_18-39-46/lightning_logs[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m [32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m   | Name | Type       | Params[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m ------------------------------------[32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m 0 | s1   | Sequential | 59.3 K[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m 59.3 K    Trainable params[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m 0         Non-trainable params[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m 59.3 K    Total params[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m 0.237     Total estimated model params size (MB)[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m   rank_zero_warn([32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000701)[32m [repeated 40x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b7401655_21_batch_size=128,layer_1_size=74,layer_2_size=240,layer_3_size=237,layer_4_size=104,layer_5_size=199,layer__2023-10-20_18-39-46/checkpoint_000013)[32m [repeated 39x across cluster][0m
[2m[36m(RayTrainWorker pid=375752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_631029cc_19_batch_size=32,layer_1_size=49,layer_2_size=179,layer_3_size=135,layer_4_size=48,layer_5_size=64,layer_6_s_2023-10-20_18-39-36/checkpoint_000013)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=377478)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=375910)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81d64aa8_20_batch_size=32,layer_1_size=249,layer_2_size=128,layer_3_size=45,layer_4_size=128,layer_5_size=143,layer_6_2023-10-20_18-39-36/checkpoint_000024)[32m [repeated 52x across cluster][0m
[2m[36m(RayTrainWorker pid=377478)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=377478)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=377478)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=377478)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=375748)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3e2b89eb_18_batch_size=128,layer_1_size=157,layer_2_size=138,layer_3_size=245,layer_4_size=219,layer_5_size=90,layer__2023-10-20_18-39-04/checkpoint_000038)[32m [repeated 40x across cluster][0m
[2m[36m(RayTrainWorker pid=377478)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e68d1447_22_batch_size=32,layer_1_size=76,layer_2_size=55,layer_3_size=37,layer_4_size=222,layer_5_size=167,layer_6_s_2023-10-20_18-39-47/lightning_logs
[2m[36m(RayTrainWorker pid=377478)[0m 
[2m[36m(RayTrainWorker pid=377478)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=377478)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=377478)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=377478)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=377478)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=377478)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=377478)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=377478)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=377478)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=377478)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=377478)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=377478)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=377478)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=377478)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=377478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e68d1447_22_batch_size=32,layer_1_size=76,layer_2_size=55,layer_3_size=37,layer_4_size=222,layer_5_size=167,layer_6_s_2023-10-20_18-39-47/checkpoint_000004)[32m [repeated 51x across cluster][0m
[2m[36m(RayTrainWorker pid=375752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_631029cc_19_batch_size=32,layer_1_size=49,layer_2_size=179,layer_3_size=135,layer_4_size=48,layer_5_size=64,layer_6_s_2023-10-20_18-39-36/checkpoint_000053)[32m [repeated 61x across cluster][0m
[2m[36m(RayTrainWorker pid=377478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e68d1447_22_batch_size=32,layer_1_size=76,layer_2_size=55,layer_3_size=37,layer_4_size=222,layer_5_size=167,layer_6_s_2023-10-20_18-39-47/checkpoint_000031)[32m [repeated 70x across cluster][0m
[2m[36m(RayTrainWorker pid=375910)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81d64aa8_20_batch_size=32,layer_1_size=249,layer_2_size=128,layer_3_size=45,layer_4_size=128,layer_5_size=143,layer_6_2023-10-20_18-39-36/checkpoint_000068)[32m [repeated 69x across cluster][0m
[2m[36m(RayTrainWorker pid=377478)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e68d1447_22_batch_size=32,layer_1_size=76,layer_2_size=55,layer_3_size=37,layer_4_size=222,layer_5_size=167,layer_6_s_2023-10-20_18-39-47/checkpoint_000059)[32m [repeated 66x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b7401655_21_batch_size=128,layer_1_size=74,layer_2_size=240,layer_3_size=237,layer_4_size=104,layer_5_size=199,layer__2023-10-20_18-39-46/checkpoint_000107)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b7401655_21_batch_size=128,layer_1_size=74,layer_2_size=240,layer_3_size=237,layer_4_size=104,layer_5_size=199,layer__2023-10-20_18-39-46/checkpoint_000110)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=375992)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b7401655_21_batch_size=128,layer_1_size=74,layer_2_size=240,layer_3_size=237,layer_4_size=104,layer_5_size=199,layer__2023-10-20_18-39-46/checkpoint_000116)[32m [repeated 12x across cluster][0m
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['378809 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000788)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000820)[32m [repeated 96x across cluster][0m
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000851)[32m [repeated 74x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['379220 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=375910)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81d64aa8_20_batch_size=32,layer_1_size=249,layer_2_size=128,layer_3_size=45,layer_4_size=128,layer_5_size=143,layer_6_2023-10-20_18-39-36/checkpoint_000145)[32m [repeated 35x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['379224 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=375992)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b7401655_21_batch_size=128,layer_1_size=74,layer_2_size=240,layer_3_size=237,layer_4_size=104,layer_5_size=199,layer__2023-10-20_18-39-46/checkpoint_000177)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=378809)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=375910)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81d64aa8_20_batch_size=32,layer_1_size=249,layer_2_size=128,layer_3_size=45,layer_4_size=128,layer_5_size=143,layer_6_2023-10-20_18-39-36/checkpoint_000156)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=375910)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81d64aa8_20_batch_size=32,layer_1_size=249,layer_2_size=128,layer_3_size=45,layer_4_size=128,layer_5_size=143,layer_6_2023-10-20_18-39-36/checkpoint_000168)[32m [repeated 24x across cluster][0m
[2m[36m(TrainTrainable pid=378810)[0m Trainable.setup took 13.752 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=378809)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=378809)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=378809)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=378809)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=375910)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81d64aa8_20_batch_size=32,layer_1_size=249,layer_2_size=128,layer_3_size=45,layer_4_size=128,layer_5_size=143,layer_6_2023-10-20_18-39-36/checkpoint_000182)[32m [repeated 46x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7f0379c9_24_batch_size=256,layer_1_size=95,layer_2_size=109,layer_3_size=102,layer_4_size=198,layer_5_size=36,layer_6_2023-10-20_18-41-22/lightning_logs
[2m[36m(RayTrainWorker pid=378809)[0m 
[2m[36m(RayTrainWorker pid=378809)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=378809)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=378809)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=378809)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=378809)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=378809)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=378809)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=378809)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=378809)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=378809)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=379224)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=379224)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(TrainTrainable pid=379221)[0m Trainable.setup took 13.758 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m GPU available: False, used: False[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m TPU available: False, using: 0 TPU cores[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m IPU available: False, using: 0 IPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m HPU available: False, using: 0 HPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379224)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_11c50faf_25_batch_size=128,layer_1_size=244,layer_2_size=59,layer_3_size=238,layer_4_size=139,layer_5_size=88,layer_6_2023-10-20_18-41-43/checkpoint_000002)[32m [repeated 49x across cluster][0m
[2m[36m(RayTrainWorker pid=378809)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0cfce45a_23_batch_size=128,layer_1_size=36,layer_2_size=207,layer_3_size=109,layer_4_size=96,layer_5_size=184,layer_6_2023-10-20_18-40-11/lightning_logs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m   | Name | Type       | Params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m ------------------------------------[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m 0 | s1   | Sequential | 173 K [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m 173 K     Trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m 0         Non-trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m 173 K     Total params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m 0.694     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=379220)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=378809)[0m   rank_zero_warn([32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=378809)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=378809)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=378809)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0cfce45a_23_batch_size=128,layer_1_size=36,layer_2_size=207,layer_3_size=109,layer_4_size=96,layer_5_size=184,layer_6_2023-10-20_18-40-11/checkpoint_000018)[32m [repeated 33x across cluster][0m
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['380062 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000934)[32m [repeated 39x across cluster][0m
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['380302 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=379220)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7f0379c9_24_batch_size=256,layer_1_size=95,layer_2_size=109,layer_3_size=102,layer_4_size=198,layer_5_size=36,layer_6_2023-10-20_18-41-22/checkpoint_000040)[32m [repeated 95x across cluster][0m
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_000999)[32m [repeated 139x across cluster][0m
[2m[36m(RayTrainWorker pid=380062)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_001050)[32m [repeated 123x across cluster][0m
[2m[36m(RayTrainWorker pid=380302)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_001114)[32m [repeated 125x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['381158 (192.0.0.1)']
[2m[36m(TrainTrainable pid=380063)[0m Trainable.setup took 11.007 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=380302)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=380302)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=380302)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=380302)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_001173)[32m [repeated 111x across cluster][0m
[2m[36m(RayTrainWorker pid=380062)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_bc059d0d_26_batch_size=64,layer_1_size=146,layer_2_size=145,layer_3_size=149,layer_4_size=122,layer_5_size=169,layer__2023-10-20_18-41-45/lightning_logs
[2m[36m(RayTrainWorker pid=380062)[0m 
[2m[36m(RayTrainWorker pid=380062)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=380062)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=380062)[0m 0 | s1   | Sequential | 59.3 K
[2m[36m(RayTrainWorker pid=380062)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=380062)[0m 59.3 K    Trainable params
[2m[36m(RayTrainWorker pid=380062)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=380062)[0m 59.3 K    Total params
[2m[36m(RayTrainWorker pid=380062)[0m 0.237     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=380062)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=380062)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=380062)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=380062)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=380062)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['381144 (192.0.0.1)']
[2m[36m(TrainTrainable pid=380310)[0m Trainable.setup took 11.008 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=380062)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=380062)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=380062)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=380062)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=380062)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_bc059d0d_26_batch_size=64,layer_1_size=146,layer_2_size=145,layer_3_size=149,layer_4_size=122,layer_5_size=169,layer__2023-10-20_18-41-45/checkpoint_000010)[32m [repeated 73x across cluster][0m
[2m[36m(RayTrainWorker pid=380302)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ebac42c0_27_batch_size=64,layer_1_size=209,layer_2_size=165,layer_3_size=170,layer_4_size=228,layer_5_size=107,layer__2023-10-20_18-42-15/lightning_logs
[2m[36m(RayTrainWorker pid=380302)[0m 
[2m[36m(RayTrainWorker pid=380302)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=380302)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=380302)[0m 0 | s1   | Sequential | 83.9 K
[2m[36m(RayTrainWorker pid=380302)[0m 83.9 K    Trainable params
[2m[36m(RayTrainWorker pid=380302)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=380302)[0m 83.9 K    Total params
[2m[36m(RayTrainWorker pid=380302)[0m 0.336     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=380302)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=380302)[0m   rank_zero_warn([32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=380302)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=380302)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=381158)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_001235)[32m [repeated 95x across cluster][0m
[2m[36m(RayTrainWorker pid=381144)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_001274)[32m [repeated 124x across cluster][0m
[2m[36m(RayTrainWorker pid=381158)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=381158)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=381158)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=381158)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=381144)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/lightning_logs
[2m[36m(RayTrainWorker pid=379224)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_11c50faf_25_batch_size=128,layer_1_size=244,layer_2_size=59,layer_3_size=238,layer_4_size=139,layer_5_size=88,layer_6_2023-10-20_18-41-43/checkpoint_000359)[32m [repeated 127x across cluster][0m
[2m[36m(RayTrainWorker pid=381158)[0m 
[2m[36m(RayTrainWorker pid=381158)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=381158)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=381158)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=381158)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=381158)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=381158)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=381158)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=381158)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=381158)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=381158)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=381158)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=381158)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=381144)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=381144)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=381144)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=381144)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=381158)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_5bec5283_29_batch_size=256,layer_1_size=100,layer_2_size=216,layer_3_size=37,layer_4_size=153,layer_5_size=219,layer__2023-10-20_18-42-43/lightning_logs
[2m[36m(RayTrainWorker pid=373631)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cd7bd8e7_15_batch_size=64,layer_1_size=62,layer_2_size=51,layer_3_size=110,layer_4_size=139,layer_5_size=152,layer_6__2023-10-20_18-38-37/checkpoint_001330)[32m [repeated 115x across cluster][0m
[2m[36m(RayTrainWorker pid=381144)[0m 
[2m[36m(RayTrainWorker pid=381144)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=381144)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=381144)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=381144)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=381144)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=381144)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=381144)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=381144)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=381144)[0m   rank_zero_warn([32m [repeated 5x across cluster][0m
[2m[36m(RayTrainWorker pid=381144)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=381144)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=380302)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ebac42c0_27_batch_size=64,layer_1_size=209,layer_2_size=165,layer_3_size=170,layer_4_size=228,layer_5_size=107,layer__2023-10-20_18-42-15/checkpoint_000110)[32m [repeated 125x across cluster][0m
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['382313 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=381158)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_5bec5283_29_batch_size=256,layer_1_size=100,layer_2_size=216,layer_3_size=37,layer_4_size=153,layer_5_size=219,layer__2023-10-20_18-42-43/checkpoint_000089)[32m [repeated 137x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['382435 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=380302)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ebac42c0_27_batch_size=64,layer_1_size=209,layer_2_size=165,layer_3_size=170,layer_4_size=228,layer_5_size=107,layer__2023-10-20_18-42-15/checkpoint_000159)[32m [repeated 104x across cluster][0m
[2m[36m(RayTrainWorker pid=381144)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/checkpoint_000137)[32m [repeated 63x across cluster][0m
[2m[36m(RayTrainWorker pid=382313)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=369350)[0m Starting distributed worker processes: ['383164 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=380302)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ebac42c0_27_batch_size=64,layer_1_size=209,layer_2_size=165,layer_3_size=170,layer_4_size=228,layer_5_size=107,layer__2023-10-20_18-42-15/checkpoint_000198)[32m [repeated 72x across cluster][0m
[2m[36m(RayTrainWorker pid=382435)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=381144)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/checkpoint_000185)[32m [repeated 42x across cluster][0m
[2m[36m(TrainTrainable pid=382316)[0m Trainable.setup took 11.577 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=382313)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=382313)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=382313)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=382313)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=382313)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/lightning_logs
[2m[36m(RayTrainWorker pid=382313)[0m 
[2m[36m(RayTrainWorker pid=382313)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=382313)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=382313)[0m 0 | s1   | Sequential | 59.3 K
[2m[36m(RayTrainWorker pid=382313)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=382313)[0m 59.3 K    Trainable params
[2m[36m(RayTrainWorker pid=382313)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=382313)[0m 59.3 K    Total params
[2m[36m(RayTrainWorker pid=382313)[0m 0.237     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=382313)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=382313)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=382313)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=382313)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=382313)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=382313)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=381144)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/checkpoint_000226)[32m [repeated 41x across cluster][0m
[2m[36m(TrainTrainable pid=382436)[0m Trainable.setup took 11.593 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=382435)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=382435)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=382435)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=382435)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=382435)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_6db529c0_31_batch_size=64,layer_1_size=59,layer_2_size=100,layer_3_size=243,layer_4_size=68,layer_5_size=209,layer_6__2023-10-20_18-43-17/lightning_logs
[2m[36m(RayTrainWorker pid=383164)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['383592 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=382435)[0m 
[2m[36m(RayTrainWorker pid=382435)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=382435)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=382435)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=382435)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=382435)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=382435)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=382435)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=382435)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=382435)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=382435)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=382435)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=382313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/checkpoint_000010)[32m [repeated 47x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['383603 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=382313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/checkpoint_000025)[32m [repeated 39x across cluster][0m
[2m[36m(RayTrainWorker pid=383164)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=383164)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=383164)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=383164)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=383164)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_54797d30_32_batch_size=64,layer_1_size=71,layer_2_size=63,layer_3_size=196,layer_4_size=111,layer_5_size=167,layer_6__2023-10-20_18-43-26/lightning_logs
[2m[36m(RayTrainWorker pid=383164)[0m 
[2m[36m(RayTrainWorker pid=383164)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=383164)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=383164)[0m 0 | s1   | Sequential | 76.4 K
[2m[36m(RayTrainWorker pid=383164)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=383164)[0m 76.4 K    Trainable params
[2m[36m(RayTrainWorker pid=383164)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=383164)[0m 76.4 K    Total params
[2m[36m(RayTrainWorker pid=383164)[0m 0.305     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=383164)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=383164)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=383164)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=383164)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=383164)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=383164)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=382435)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_6db529c0_31_batch_size=64,layer_1_size=59,layer_2_size=100,layer_3_size=243,layer_4_size=68,layer_5_size=209,layer_6__2023-10-20_18-43-17/checkpoint_000056)[32m [repeated 56x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=381144)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/checkpoint_000279)[32m [repeated 47x across cluster][0m
[2m[36m(RayTrainWorker pid=383603)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=383592)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=383592)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=383592)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=383592)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=383603)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_27371204_34_batch_size=64,layer_1_size=241,layer_2_size=38,layer_3_size=143,layer_4_size=77,layer_5_size=70,layer_6_s_2023-10-20_18-43-48/lightning_logs
[2m[36m(RayTrainWorker pid=382313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/checkpoint_000074)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m 
[2m[36m(RayTrainWorker pid=383592)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=383592)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=383592)[0m 0 | s1   | Sequential | 83.9 K
[2m[36m(RayTrainWorker pid=383592)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=383592)[0m 83.9 K    Trainable params
[2m[36m(RayTrainWorker pid=383592)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=383592)[0m 83.9 K    Total params
[2m[36m(RayTrainWorker pid=383592)[0m 0.336     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=383592)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=383592)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=383592)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=383592)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=383592)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=383592)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=383603)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=383603)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=383603)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=383603)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=383592)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/lightning_logs
[2m[36m(RayTrainWorker pid=383164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_54797d30_32_batch_size=64,layer_1_size=71,layer_2_size=63,layer_3_size=196,layer_4_size=111,layer_5_size=167,layer_6__2023-10-20_18-43-26/checkpoint_000038)[32m [repeated 48x across cluster][0m
[2m[36m(RayTrainWorker pid=383603)[0m 
[2m[36m(RayTrainWorker pid=383603)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=383603)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=383603)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=383603)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=383603)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=383603)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=383603)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=383603)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=383603)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=383603)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=383603)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000019)[32m [repeated 41x across cluster][0m
[2m[36m(TorchTrainer pid=383605)[0m Starting distributed worker processes: ['384841 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=383164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_54797d30_32_batch_size=64,layer_1_size=71,layer_2_size=63,layer_3_size=196,layer_4_size=111,layer_5_size=167,layer_6__2023-10-20_18-43-26/checkpoint_000063)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=383164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_54797d30_32_batch_size=64,layer_1_size=71,layer_2_size=63,layer_3_size=196,layer_4_size=111,layer_5_size=167,layer_6__2023-10-20_18-43-26/checkpoint_000071)[32m [repeated 45x across cluster][0m
[2m[36m(RayTrainWorker pid=384841)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=383164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_54797d30_32_batch_size=64,layer_1_size=71,layer_2_size=63,layer_3_size=196,layer_4_size=111,layer_5_size=167,layer_6__2023-10-20_18-43-26/checkpoint_000078)[32m [repeated 37x across cluster][0m
[2m[36m(RayTrainWorker pid=381144)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/checkpoint_000326)[32m [repeated 47x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['385292 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=381144)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/checkpoint_000339)[32m [repeated 64x across cluster][0m
[2m[36m(TrainTrainable pid=384842)[0m Trainable.setup took 12.589 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=384841)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=384841)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=384841)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=384841)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=384841)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_16cfde1f_35_batch_size=64,layer_1_size=102,layer_2_size=247,layer_3_size=58,layer_4_size=149,layer_5_size=182,layer_6_2023-10-20_18-43-53/lightning_logs
[2m[36m(RayTrainWorker pid=381144)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/checkpoint_000375)[32m [repeated 100x across cluster][0m
[2m[36m(RayTrainWorker pid=384841)[0m 
[2m[36m(RayTrainWorker pid=384841)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=384841)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=384841)[0m 0 | s1   | Sequential | 99.7 K
[2m[36m(RayTrainWorker pid=384841)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=384841)[0m 99.7 K    Trainable params
[2m[36m(RayTrainWorker pid=384841)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=384841)[0m 99.7 K    Total params
[2m[36m(RayTrainWorker pid=384841)[0m 0.399     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=384841)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=384841)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=384841)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=384841)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=384841)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=384841)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=381144)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32ce59a0_28_batch_size=64,layer_1_size=76,layer_2_size=243,layer_3_size=92,layer_4_size=153,layer_5_size=212,layer_6__2023-10-20_18-42-20/checkpoint_000384)[32m [repeated 63x across cluster][0m
[2m[36m(RayTrainWorker pid=385292)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=369350)[0m Starting distributed worker processes: ['385766 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000121)[32m [repeated 46x across cluster][0m
[2m[36m(RayTrainWorker pid=383603)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_27371204_34_batch_size=64,layer_1_size=241,layer_2_size=38,layer_3_size=143,layer_4_size=77,layer_5_size=70,layer_6_s_2023-10-20_18-43-48/checkpoint_000087)[32m [repeated 63x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000163)[32m [repeated 101x across cluster][0m
[2m[36m(TrainTrainable pid=385293)[0m Trainable.setup took 15.858 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=385292)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=385292)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=385292)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=385292)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000184)[32m [repeated 63x across cluster][0m
[2m[36m(RayTrainWorker pid=385292)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_24c502da_36_batch_size=256,layer_1_size=133,layer_2_size=143,layer_3_size=178,layer_4_size=47,layer_5_size=237,layer__2023-10-20_18-44-23/lightning_logs
[2m[36m(RayTrainWorker pid=385766)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['386285 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=385292)[0m 
[2m[36m(RayTrainWorker pid=385292)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=385292)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=385292)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=385292)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=385292)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=385292)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=385292)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=385292)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=385292)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=385292)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=385292)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=385292)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=385292)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=385292)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=382313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/checkpoint_000265)[32m [repeated 25x across cluster][0m
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['386273 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=385292)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_24c502da_36_batch_size=256,layer_1_size=133,layer_2_size=143,layer_3_size=178,layer_4_size=47,layer_5_size=237,layer__2023-10-20_18-44-23/checkpoint_000008)[32m [repeated 31x across cluster][0m
[2m[36m(TrainTrainable pid=385767)[0m Trainable.setup took 11.452 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=385766)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=385766)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=385766)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=385766)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=385766)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/lightning_logs
[2m[36m(RayTrainWorker pid=384841)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_16cfde1f_35_batch_size=64,layer_1_size=102,layer_2_size=247,layer_3_size=58,layer_4_size=149,layer_5_size=182,layer_6_2023-10-20_18-43-53/checkpoint_000094)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=385766)[0m 
[2m[36m(RayTrainWorker pid=385766)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=385766)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=385766)[0m 0 | s1   | Sequential | 76.4 K
[2m[36m(RayTrainWorker pid=385766)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=385766)[0m 76.4 K    Trainable params
[2m[36m(RayTrainWorker pid=385766)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=385766)[0m 76.4 K    Total params
[2m[36m(RayTrainWorker pid=385766)[0m 0.305     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=385766)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=385766)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=385766)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=385766)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=385766)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=385766)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=386285)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=385292)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_24c502da_36_batch_size=256,layer_1_size=133,layer_2_size=143,layer_3_size=178,layer_4_size=47,layer_5_size=237,layer__2023-10-20_18-44-23/checkpoint_000027)[32m [repeated 36x across cluster][0m
[2m[36m(RayTrainWorker pid=386273)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000020)[32m [repeated 45x across cluster][0m
[2m[36m(RayTrainWorker pid=386285)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=386285)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=386285)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=386285)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=386285)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76a28f36_39_batch_size=32,layer_1_size=211,layer_2_size=174,layer_3_size=190,layer_4_size=136,layer_5_size=78,layer_6_2023-10-20_18-45-16/lightning_logs
[2m[36m(RayTrainWorker pid=386285)[0m 
[2m[36m(RayTrainWorker pid=386285)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=386285)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=386285)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=386285)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=386285)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=386285)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=386285)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=386285)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=386285)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=386285)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=386285)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=386285)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=385292)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_24c502da_36_batch_size=256,layer_1_size=133,layer_2_size=143,layer_3_size=178,layer_4_size=47,layer_5_size=237,layer__2023-10-20_18-44-23/checkpoint_000064)[32m [repeated 95x across cluster][0m
[2m[36m(RayTrainWorker pid=386273)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=386273)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=386273)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=386273)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=386273)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_10a751cc_38_batch_size=256,layer_1_size=243,layer_2_size=249,layer_3_size=58,layer_4_size=167,layer_5_size=176,layer__2023-10-20_18-45-01/lightning_logs
[2m[36m(RayTrainWorker pid=386273)[0m 
[2m[36m(RayTrainWorker pid=386273)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=386273)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=386273)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=386273)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=386273)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=386273)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=386273)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=386273)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=386273)[0m   rank_zero_warn([32m [repeated 5x across cluster][0m
[2m[36m(RayTrainWorker pid=386273)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=386273)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=382313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/checkpoint_000318)[32m [repeated 96x across cluster][0m
[2m[36m(RayTrainWorker pid=385292)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_24c502da_36_batch_size=256,layer_1_size=133,layer_2_size=143,layer_3_size=178,layer_4_size=47,layer_5_size=237,layer__2023-10-20_18-44-23/checkpoint_000100)[32m [repeated 101x across cluster][0m
[2m[36m(TorchTrainer pid=383605)[0m Starting distributed worker processes: ['387299 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=382313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/checkpoint_000341)[32m [repeated 93x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['387886 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=382313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/checkpoint_000360)[32m [repeated 96x across cluster][0m
[2m[36m(RayTrainWorker pid=382313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_374c4e3f_30_batch_size=128,layer_1_size=186,layer_2_size=147,layer_3_size=48,layer_4_size=83,layer_5_size=104,layer_6_2023-10-20_18-42-46/checkpoint_000375)[32m [repeated 93x across cluster][0m
[2m[36m(RayTrainWorker pid=387299)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=386285)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76a28f36_39_batch_size=32,layer_1_size=211,layer_2_size=174,layer_3_size=190,layer_4_size=136,layer_5_size=78,layer_6_2023-10-20_18-45-16/checkpoint_000081)[32m [repeated 51x across cluster][0m
[2m[36m(RayTrainWorker pid=387886)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['388296 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=386285)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76a28f36_39_batch_size=32,layer_1_size=211,layer_2_size=174,layer_3_size=190,layer_4_size=136,layer_5_size=78,layer_6_2023-10-20_18-45-16/checkpoint_000090)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000171)[32m [repeated 28x across cluster][0m
[2m[36m(TrainTrainable pid=387300)[0m Trainable.setup took 14.250 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=387299)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=387299)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=387299)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=387299)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=387886)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_5028c3ff_41_batch_size=32,layer_1_size=249,layer_2_size=41,layer_3_size=135,layer_4_size=32,layer_5_size=249,layer_6__2023-10-20_18-45-55/lightning_logs
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000417)[32m [repeated 64x across cluster][0m
[2m[36m(RayTrainWorker pid=387299)[0m 
[2m[36m(RayTrainWorker pid=387299)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=387299)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=387299)[0m 0 | s1   | Sequential | 99.7 K
[2m[36m(RayTrainWorker pid=387299)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=387299)[0m 99.7 K    Trainable params
[2m[36m(RayTrainWorker pid=387299)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=387299)[0m 99.7 K    Total params
[2m[36m(RayTrainWorker pid=387299)[0m 0.399     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=387886)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=387886)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=387886)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=387886)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=387886)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=387886)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=388296)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TrainTrainable pid=387887)[0m Trainable.setup took 14.427 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=387886)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=387886)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=387886)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=387886)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=387299)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b304ced6_40_batch_size=64,layer_1_size=150,layer_2_size=177,layer_3_size=111,layer_4_size=123,layer_5_size=60,layer_6_2023-10-20_18-45-22/lightning_logs
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000224)[32m [repeated 69x across cluster][0m
[2m[36m(RayTrainWorker pid=387886)[0m 
[2m[36m(RayTrainWorker pid=387886)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=387886)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=387886)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=387886)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=387886)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=387886)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=387886)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=387299)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=387299)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=387299)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=387299)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['388752 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=387299)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b304ced6_40_batch_size=64,layer_1_size=150,layer_2_size=177,layer_3_size=111,layer_4_size=123,layer_5_size=60,layer_6_2023-10-20_18-45-22/checkpoint_000010)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=388296)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=388296)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=388296)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=388296)[0m HPU available: False, using: 0 HPUs
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['388756 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=388296)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_56bce9a3_42_batch_size=32,layer_1_size=116,layer_2_size=168,layer_3_size=143,layer_4_size=146,layer_5_size=231,layer__2023-10-20_18-46-11/lightning_logs
[2m[36m(RayTrainWorker pid=387886)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_5028c3ff_41_batch_size=32,layer_1_size=249,layer_2_size=41,layer_3_size=135,layer_4_size=32,layer_5_size=249,layer_6__2023-10-20_18-45-55/checkpoint_000034)[32m [repeated 39x across cluster][0m
[2m[36m(RayTrainWorker pid=388296)[0m 
[2m[36m(RayTrainWorker pid=388296)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=388296)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=388296)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=388296)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=388296)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=388296)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=388296)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=388296)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=388296)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=388296)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=388296)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=388296)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=388296)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=388296)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000253)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=388756)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=387886)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_5028c3ff_41_batch_size=32,layer_1_size=249,layer_2_size=41,layer_3_size=135,layer_4_size=32,layer_5_size=249,layer_6__2023-10-20_18-45-55/checkpoint_000041)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=388752)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=388756)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=388756)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=388756)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=388756)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000480)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=388756)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc3913b5_44_batch_size=64,layer_1_size=100,layer_2_size=144,layer_3_size=163,layer_4_size=32,layer_5_size=102,layer_6_2023-10-20_18-46-41/lightning_logs
[2m[36m(RayTrainWorker pid=388756)[0m 
[2m[36m(RayTrainWorker pid=388756)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=388756)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=388756)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=388756)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=388756)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=388756)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=388756)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=388756)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=388756)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=388756)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=388756)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=388756)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=388756)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=388756)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=388752)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=388752)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=388752)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=388752)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000263)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=388752)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_37ba61b0_43_batch_size=256,layer_1_size=32,layer_2_size=152,layer_3_size=249,layer_4_size=166,layer_5_size=249,layer__2023-10-20_18-46-21/lightning_logs
[2m[36m(RayTrainWorker pid=388752)[0m 
[2m[36m(RayTrainWorker pid=388752)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=388752)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=388752)[0m 0 | s1   | Sequential | 59.3 K
[2m[36m(RayTrainWorker pid=388752)[0m 59.3 K    Trainable params
[2m[36m(RayTrainWorker pid=388752)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=388752)[0m 59.3 K    Total params
[2m[36m(RayTrainWorker pid=388752)[0m 0.237     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=388752)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=388752)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=388752)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=388752)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=388752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_37ba61b0_43_batch_size=256,layer_1_size=32,layer_2_size=152,layer_3_size=249,layer_4_size=166,layer_5_size=249,layer__2023-10-20_18-46-21/checkpoint_000011)[32m [repeated 26x across cluster][0m
[2m[36m(TorchTrainer pid=388757)[0m Starting distributed worker processes: ['389992 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=387299)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b304ced6_40_batch_size=64,layer_1_size=150,layer_2_size=177,layer_3_size=111,layer_4_size=123,layer_5_size=60,layer_6_2023-10-20_18-45-22/checkpoint_000041)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000271)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=388752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_37ba61b0_43_batch_size=256,layer_1_size=32,layer_2_size=152,layer_3_size=249,layer_4_size=166,layer_5_size=249,layer__2023-10-20_18-46-21/checkpoint_000026)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=389992)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000276)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=388752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_37ba61b0_43_batch_size=256,layer_1_size=32,layer_2_size=152,layer_3_size=249,layer_4_size=166,layer_5_size=249,layer__2023-10-20_18-46-21/checkpoint_000030)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000278)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=388296)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_56bce9a3_42_batch_size=32,layer_1_size=116,layer_2_size=168,layer_3_size=143,layer_4_size=146,layer_5_size=231,layer__2023-10-20_18-46-11/checkpoint_000042)[32m [repeated 22x across cluster][0m
[2m[36m(TrainTrainable pid=389993)[0m Trainable.setup took 18.058 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=389992)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=389992)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=389992)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=389992)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=389992)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_378de333_45_batch_size=256,layer_1_size=107,layer_2_size=39,layer_3_size=126,layer_4_size=125,layer_5_size=154,layer__2023-10-20_18-46-41/lightning_logs
[2m[36m(RayTrainWorker pid=388752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_37ba61b0_43_batch_size=256,layer_1_size=32,layer_2_size=152,layer_3_size=249,layer_4_size=166,layer_5_size=249,layer__2023-10-20_18-46-21/checkpoint_000035)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=389992)[0m 
[2m[36m(RayTrainWorker pid=389992)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=389992)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=389992)[0m 0 | s1   | Sequential | 138 K 
[2m[36m(RayTrainWorker pid=389992)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=389992)[0m 138 K     Trainable params
[2m[36m(RayTrainWorker pid=389992)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=389992)[0m 138 K     Total params
[2m[36m(RayTrainWorker pid=389992)[0m 0.556     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=389992)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=389992)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=389992)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=389992)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=389992)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=389992)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000287)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=388296)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_56bce9a3_42_batch_size=32,layer_1_size=116,layer_2_size=168,layer_3_size=143,layer_4_size=146,layer_5_size=231,layer__2023-10-20_18-46-11/checkpoint_000051)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000298)[32m [repeated 59x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000524)[32m [repeated 87x across cluster][0m
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['390607 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=388752)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_37ba61b0_43_batch_size=256,layer_1_size=32,layer_2_size=152,layer_3_size=249,layer_4_size=166,layer_5_size=249,layer__2023-10-20_18-46-21/checkpoint_000073)[32m [repeated 74x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['390946 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000331)[32m [repeated 59x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000553)[32m [repeated 48x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000561)[32m [repeated 39x across cluster][0m
[2m[36m(TorchTrainer pid=383605)[0m Starting distributed worker processes: ['391420 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=390607)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=388756)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc3913b5_44_batch_size=64,layer_1_size=100,layer_2_size=144,layer_3_size=163,layer_4_size=32,layer_5_size=102,layer_6_2023-10-20_18-46-41/checkpoint_000096)[32m [repeated 39x across cluster][0m
[2m[36m(RayTrainWorker pid=390946)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['391834 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=389992)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_378de333_45_batch_size=256,layer_1_size=107,layer_2_size=39,layer_3_size=126,layer_4_size=125,layer_5_size=154,layer__2023-10-20_18-46-41/checkpoint_000076)[32m [repeated 37x across cluster][0m
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['391766 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000367)[32m [repeated 41x across cluster][0m
[2m[36m(TrainTrainable pid=390608)[0m Trainable.setup took 14.964 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=390607)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=390607)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=390607)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=390607)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=391766)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=390607)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7b6377c6_46_batch_size=256,layer_1_size=131,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=147,layer__2023-10-20_18-47-17/lightning_logs
[2m[36m(RayTrainWorker pid=385766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c9193c2a_37_batch_size=256,layer_1_size=73,layer_2_size=128,layer_3_size=43,layer_4_size=42,layer_5_size=139,layer_6__2023-10-20_18-44-46/checkpoint_000375)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=390607)[0m 
[2m[36m(RayTrainWorker pid=390607)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=390607)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=390607)[0m 0 | s1   | Sequential | 156 K 
[2m[36m(RayTrainWorker pid=390607)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=390607)[0m 156 K     Trainable params
[2m[36m(RayTrainWorker pid=390607)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=390607)[0m 156 K     Total params
[2m[36m(RayTrainWorker pid=390607)[0m 0.627     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=390607)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=390607)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=390607)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=390607)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(TrainTrainable pid=390947)[0m Trainable.setup took 14.983 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=390946)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=390946)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=390946)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=390946)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=391834)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=390946)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_a451589d_47_batch_size=256,layer_1_size=101,layer_2_size=138,layer_3_size=154,layer_4_size=138,layer_5_size=127,layer_2023-10-20_18-48-14/lightning_logs
[2m[36m(RayTrainWorker pid=388756)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc3913b5_44_batch_size=64,layer_1_size=100,layer_2_size=144,layer_3_size=163,layer_4_size=32,layer_5_size=102,layer_6_2023-10-20_18-46-41/checkpoint_000146)[32m [repeated 78x across cluster][0m
[2m[36m(RayTrainWorker pid=390946)[0m 
[2m[36m(RayTrainWorker pid=390946)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=390946)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=390946)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=390946)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=390946)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=390946)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=390946)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=390946)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=390946)[0m   rank_zero_warn([32m [repeated 5x across cluster][0m
[2m[36m(RayTrainWorker pid=390946)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=390946)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=391766)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=391766)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=391766)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=391766)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=390607)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7b6377c6_46_batch_size=256,layer_1_size=131,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=147,layer__2023-10-20_18-47-17/checkpoint_000028)[32m [repeated 76x across cluster][0m
[2m[36m(RayTrainWorker pid=391766)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c7b351f8_49_batch_size=32,layer_1_size=114,layer_2_size=249,layer_3_size=185,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_18-48-39/lightning_logs
[2m[36m(RayTrainWorker pid=391766)[0m 
[2m[36m(RayTrainWorker pid=391766)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=391766)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=391766)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=391766)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=391766)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=391766)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=391766)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=391766)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=391766)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=391834)[0m ------------------------------------[32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m   rank_zero_warn([32m [repeated 7x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m GPU available: False, used: False[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m TPU available: False, using: 0 TPU cores[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m IPU available: False, using: 0 IPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m HPU available: False, using: 0 HPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=388756)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc3913b5_44_batch_size=64,layer_1_size=100,layer_2_size=144,layer_3_size=163,layer_4_size=32,layer_5_size=102,layer_6_2023-10-20_18-46-41/checkpoint_000179)[32m [repeated 76x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3eae5056_50_batch_size=32,layer_1_size=133,layer_2_size=249,layer_3_size=140,layer_4_size=77,layer_5_size=159,layer_6_2023-10-20_18-48-49/lightning_logs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m   | Name | Type       | Params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m 0 | s1   | Sequential | 59.3 K[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m 59.3 K    Trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m 0         Non-trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m 59.3 K    Total params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m 0.237     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m   rank_zero_warn([32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=390946)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_a451589d_47_batch_size=256,layer_1_size=101,layer_2_size=138,layer_3_size=154,layer_4_size=138,layer_5_size=127,layer_2023-10-20_18-48-14/checkpoint_000052)[32m [repeated 78x across cluster][0m
[2m[36m(TorchTrainer pid=369350)[0m Starting distributed worker processes: ['393000 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=388756)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc3913b5_44_batch_size=64,layer_1_size=100,layer_2_size=144,layer_3_size=163,layer_4_size=32,layer_5_size=102,layer_6_2023-10-20_18-46-41/checkpoint_000187)[32m [repeated 61x across cluster][0m
[2m[36m(TorchTrainer pid=388757)[0m Starting distributed worker processes: ['392984 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=391834)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3eae5056_50_batch_size=32,layer_1_size=133,layer_2_size=249,layer_3_size=140,layer_4_size=77,layer_5_size=159,layer_6_2023-10-20_18-48-49/checkpoint_000031)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=390946)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_a451589d_47_batch_size=256,layer_1_size=101,layer_2_size=138,layer_3_size=154,layer_4_size=138,layer_5_size=127,layer_2023-10-20_18-48-14/checkpoint_000069)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000689)[32m [repeated 48x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=392984)[0m [W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:58073 (errno: 98 - Address already in use).
[2m[36m(RayTrainWorker pid=392984)[0m [W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:58073 (errno: 98 - Address already in use).
[2m[36m(RayTrainWorker pid=392984)[0m [E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
[2m[36m(RayTrainWorker pid=391766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c7b351f8_49_batch_size=32,layer_1_size=114,layer_2_size=249,layer_3_size=185,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_18-48-39/checkpoint_000049)[32m [repeated 55x across cluster][0m
2023-10-20 18:49:55,889	ERROR tune_controller.py:1502 -- Trial task failed for trial TorchTrainer_5e50ed6e
Traceback (most recent call last):
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2547, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): [36mray::_Inner.train()[39m (pid=388757, ip=192.0.0.1, actor_id=cc57f52e7d79f0633ce0149901000000, repr=TorchTrainer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/trainable.py", line 400, in train
    raise skipped from exception_cause(skipped)
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/air/_internal/util.py", line 91, in run
    self._ret = self._target(*self._args, **self._kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py", line 716, in <lambda>
    training_func=lambda: self._trainable_func(self.config),
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py", line 856, in _trainable_func
    super()._trainable_func(self._merged_config)
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py", line 822, in _trainable_func
    output = fn()
             ^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py", line 766, in train_func
    trainer.training_loop()
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/train/data_parallel_trainer.py", line 543, in training_loop
    backend_executor.start(initialization_hook=clear_lazy_checkpoint_marker)
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/train/_internal/backend_executor.py", line 156, in start
    self._backend.on_start(self.worker_group, self._backend_config)
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/train/torch/config.py", line 199, in on_start
    ray.get(setup_futures)
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(RuntimeError): [36mray::_RayTrainWorker__execute._setup_torch_process_group()[39m (pid=392984, ip=192.0.0.1, actor_id=faff56278a86947db38bf42b01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x145736d27590>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/train/_internal/worker_group.py", line 30, in __execute
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/ray/train/torch/config.py", line 107, in _setup_torch_process_group
    dist.init_process_group(
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 900, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 245, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/torch/distributed/rendezvous.py", line 176, in _create_c10d_store
    return TCPStore(
           ^^^^^^^^^
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:58073 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:58073 (errno: 98 - Address already in use).
[2m[36m(RayTrainWorker pid=392984)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=390607)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7b6377c6_46_batch_size=256,layer_1_size=131,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=147,layer__2023-10-20_18-47-17/checkpoint_000081)[32m [repeated 50x across cluster][0m
[2m[36m(RayTrainWorker pid=391420)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_03e1d958_48_batch_size=32,layer_1_size=128,layer_2_size=249,layer_3_size=168,layer_4_size=82,layer_5_size=249,layer_6_2023-10-20_18-48-29/checkpoint_000055)[32m [repeated 21x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['394091 (192.0.0.1)']
[2m[36m(TrainTrainable pid=393003)[0m Trainable.setup took 13.818 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=393000)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=393000)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=393000)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=393000)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000718)[32m [repeated 31x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/lightning_logs
[2m[36m(RayTrainWorker pid=393000)[0m 
[2m[36m(RayTrainWorker pid=393000)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=393000)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=393000)[0m 0 | s1   | Sequential | 76.4 K
[2m[36m(RayTrainWorker pid=393000)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=393000)[0m 76.4 K    Trainable params
[2m[36m(RayTrainWorker pid=393000)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=393000)[0m 76.4 K    Total params
[2m[36m(RayTrainWorker pid=393000)[0m 0.305     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=393000)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=393000)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=393000)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=393000)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=393000)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=393000)[0m   rank_zero_warn(
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['394179 (192.0.0.1)']
[2m[36m(TrainTrainable pid=392991)[0m Trainable.setup took 13.818 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=391420)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_03e1d958_48_batch_size=32,layer_1_size=128,layer_2_size=249,layer_3_size=168,layer_4_size=82,layer_5_size=249,layer_6_2023-10-20_18-48-29/checkpoint_000070)[32m [repeated 31x across cluster][0m
[2m[36m(RayTrainWorker pid=391766)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c7b351f8_49_batch_size=32,layer_1_size=114,layer_2_size=249,layer_3_size=185,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_18-48-39/checkpoint_000085)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000017)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=394091)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['395074 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000738)[32m [repeated 48x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000037)[32m [repeated 48x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000055)[32m [repeated 45x across cluster][0m
[2m[36m(TrainTrainable pid=394090)[0m Trainable.setup took 13.128 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=394091)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=394091)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=394091)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=394091)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=394091)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c47eccf5_53_batch_size=256,layer_1_size=32,layer_2_size=94,layer_3_size=109,layer_4_size=134,layer_5_size=217,layer_6_2023-10-20_18-49-17/lightning_logs
[2m[36m(RayTrainWorker pid=395074)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=394091)[0m 
[2m[36m(RayTrainWorker pid=394091)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=394091)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=394091)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=394091)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=394091)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=394091)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=394091)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=394091)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=394091)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=394091)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=394091)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=394091)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=394091)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=394091)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000065)[32m [repeated 29x across cluster][0m
[2m[36m(TrainTrainable pid=394180)[0m Trainable.setup took 13.128 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=394179)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=394179)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=394179)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=394179)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=394179)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/lightning_logs
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['395498 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=394179)[0m 
[2m[36m(RayTrainWorker pid=394179)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=394179)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=394179)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=394179)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=394179)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=394179)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=394179)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=394179)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=394179)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000778)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=395074)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=395074)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=395074)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=395074)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=395074)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32b594d6_55_batch_size=32,layer_1_size=168,layer_2_size=191,layer_3_size=186,layer_4_size=249,layer_5_size=249,layer__2023-10-20_18-50-03/lightning_logs
[2m[36m(RayTrainWorker pid=395074)[0m 
[2m[36m(RayTrainWorker pid=395074)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=395074)[0m 0 | s1   | Sequential | 156 K 
[2m[36m(RayTrainWorker pid=395074)[0m 156 K     Trainable params
[2m[36m(RayTrainWorker pid=395074)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=395074)[0m 156 K     Total params
[2m[36m(RayTrainWorker pid=395074)[0m 0.627     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=395074)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=395074)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=395074)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=395074)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=395074)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3eae5056_50_batch_size=32,layer_1_size=133,layer_2_size=249,layer_3_size=140,layer_4_size=77,layer_5_size=159,layer_6_2023-10-20_18-48-49/checkpoint_000109)[32m [repeated 51x across cluster][0m
[2m[36m(RayTrainWorker pid=394091)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c47eccf5_53_batch_size=256,layer_1_size=32,layer_2_size=94,layer_3_size=109,layer_4_size=134,layer_5_size=217,layer_6_2023-10-20_18-49-17/checkpoint_000024)[32m [repeated 61x across cluster][0m
[2m[36m(RayTrainWorker pid=395498)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=391834)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3eae5056_50_batch_size=32,layer_1_size=133,layer_2_size=249,layer_3_size=140,layer_4_size=77,layer_5_size=159,layer_6_2023-10-20_18-48-49/checkpoint_000120)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=395498)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=395498)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=395498)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=395498)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=394091)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c47eccf5_53_batch_size=256,layer_1_size=32,layer_2_size=94,layer_3_size=109,layer_4_size=134,layer_5_size=217,layer_6_2023-10-20_18-49-17/checkpoint_000035)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=395498)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_dd77fc10_56_batch_size=32,layer_1_size=230,layer_2_size=80,layer_3_size=161,layer_4_size=138,layer_5_size=249,layer_6_2023-10-20_18-50-15/lightning_logs
[2m[36m(RayTrainWorker pid=395498)[0m 
[2m[36m(RayTrainWorker pid=395498)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=395498)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=395498)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=395498)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=395498)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=395498)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=395498)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=395498)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=395498)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=395498)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=395498)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=395498)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=395498)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=395498)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=391834)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3eae5056_50_batch_size=32,layer_1_size=133,layer_2_size=249,layer_3_size=140,layer_4_size=77,layer_5_size=159,layer_6_2023-10-20_18-48-49/checkpoint_000126)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=395498)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_dd77fc10_56_batch_size=32,layer_1_size=230,layer_2_size=80,layer_3_size=161,layer_4_size=138,layer_5_size=249,layer_6_2023-10-20_18-50-15/checkpoint_000011)[32m [repeated 43x across cluster][0m
[2m[36m(TorchTrainer pid=395499)[0m Starting distributed worker processes: ['396256 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000049)[32m [repeated 43x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000093)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=391420)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_03e1d958_48_batch_size=32,layer_1_size=128,layer_2_size=249,layer_3_size=168,layer_4_size=82,layer_5_size=249,layer_6_2023-10-20_18-48-29/checkpoint_000174)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=391834)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3eae5056_50_batch_size=32,layer_1_size=133,layer_2_size=249,layer_3_size=140,layer_4_size=77,layer_5_size=159,layer_6_2023-10-20_18-48-49/checkpoint_000151)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=395074)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32b594d6_55_batch_size=32,layer_1_size=168,layer_2_size=191,layer_3_size=186,layer_4_size=249,layer_5_size=249,layer__2023-10-20_18-50-03/checkpoint_000060)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=395074)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32b594d6_55_batch_size=32,layer_1_size=168,layer_2_size=191,layer_3_size=186,layer_4_size=249,layer_5_size=249,layer__2023-10-20_18-50-03/checkpoint_000063)[32m [repeated 35x across cluster][0m
[2m[36m(RayTrainWorker pid=395074)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32b594d6_55_batch_size=32,layer_1_size=168,layer_2_size=191,layer_3_size=186,layer_4_size=249,layer_5_size=249,layer__2023-10-20_18-50-03/checkpoint_000071)[32m [repeated 34x across cluster][0m
[2m[36m(TrainTrainable pid=396257)[0m Trainable.setup took 14.500 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=396256)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=396256)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=396256)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=396256)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=396256)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/lightning_logs
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000119)[32m [repeated 46x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m 
[2m[36m(RayTrainWorker pid=396256)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=396256)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=396256)[0m 0 | s1   | Sequential | 160 K 
[2m[36m(RayTrainWorker pid=396256)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=396256)[0m 160 K     Trainable params
[2m[36m(RayTrainWorker pid=396256)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=396256)[0m 160 K     Total params
[2m[36m(RayTrainWorker pid=396256)[0m 0.643     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=396256)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=396256)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=396256)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=396256)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=396256)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=396256)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=395498)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_dd77fc10_56_batch_size=32,layer_1_size=230,layer_2_size=80,layer_3_size=161,layer_4_size=138,layer_5_size=249,layer_6_2023-10-20_18-50-15/checkpoint_000045)[32m [repeated 52x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000866)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000132)[32m [repeated 20x across cluster][0m
[2m[36m(TorchTrainer pid=383605)[0m Starting distributed worker processes: ['396797 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000096)[32m [repeated 18x across cluster][0m
[2m[36m(TorchTrainer pid=396257)[0m Starting distributed worker processes: ['396762 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000015)[32m [repeated 32x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['397587 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000147)[32m [repeated 47x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000121)[32m [repeated 49x across cluster][0m
[2m[36m(RayTrainWorker pid=396762)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['397942 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000906)[32m [repeated 62x across cluster][0m
[2m[36m(RayTrainWorker pid=396797)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000058)[32m [repeated 57x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000151)[32m [repeated 52x across cluster][0m
[2m[36m(TrainTrainable pid=396763)[0m Trainable.setup took 15.051 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=396762)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=396762)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=396762)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=396762)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=396762)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cf41ad0c_58_batch_size=256,layer_1_size=144,layer_2_size=249,layer_3_size=158,layer_4_size=168,layer_5_size=74,layer__2023-10-20_18-51-11/lightning_logs
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['398445 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000161)[32m [repeated 56x across cluster][0m
[2m[36m(RayTrainWorker pid=396762)[0m 
[2m[36m(RayTrainWorker pid=396762)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=396762)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=396762)[0m 0 | s1   | Sequential | 196 K 
[2m[36m(RayTrainWorker pid=396762)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=396762)[0m 196 K     Trainable params
[2m[36m(RayTrainWorker pid=396762)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=396762)[0m 196 K     Total params
[2m[36m(RayTrainWorker pid=396762)[0m 0.787     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=396762)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=396762)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=396762)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=396762)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=396762)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=396762)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=397942)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TrainTrainable pid=397588)[0m Trainable.setup took 15.057 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m GPU available: False, used: False[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m TPU available: False, using: 0 TPU cores[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m IPU available: False, using: 0 IPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m HPU available: False, using: 0 HPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ed70a45e_60_batch_size=128,layer_1_size=227,layer_2_size=32,layer_3_size=32,layer_4_size=169,layer_5_size=198,layer_6_2023-10-20_18-52-10/lightning_logs[32m [repeated 2x across cluster][0m
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['398438 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_000960)[32m [repeated 57x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m   | Name | Type       | Params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m ------------------------------------[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m 0 | s1   | Sequential | 173 K [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m 173 K     Trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m 0         Non-trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m 173 K     Total params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m 0.694     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m   rank_zero_warn([32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000234)[32m [repeated 60x across cluster][0m
[2m[36m(RayTrainWorker pid=398438)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TrainTrainable pid=398007)[0m Trainable.setup took 10.314 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=397942)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=397942)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=397942)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=397942)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=397942)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f6a500cf_61_batch_size=32,layer_1_size=32,layer_2_size=146,layer_3_size=249,layer_4_size=32,layer_5_size=32,layer_6_s_2023-10-20_18-52-19/lightning_logs
[2m[36m(RayTrainWorker pid=397942)[0m 
[2m[36m(RayTrainWorker pid=397942)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=397942)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=397942)[0m 0 | s1   | Sequential | 59.3 K
[2m[36m(RayTrainWorker pid=397942)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=397942)[0m 59.3 K    Trainable params
[2m[36m(RayTrainWorker pid=397942)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=397942)[0m 59.3 K    Total params
[2m[36m(RayTrainWorker pid=397942)[0m 0.237     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=397942)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=397942)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=397942)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=397942)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=397942)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=397942)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=396762)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cf41ad0c_58_batch_size=256,layer_1_size=144,layer_2_size=249,layer_3_size=158,layer_4_size=168,layer_5_size=74,layer__2023-10-20_18-51-11/checkpoint_000026)[32m [repeated 57x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=398438)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=398438)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=398438)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=398438)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=398445)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=398445)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=398445)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=398445)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=398438)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3c14bfb3_62_batch_size=32,layer_1_size=249,layer_2_size=209,layer_3_size=249,layer_4_size=249,layer_5_size=130,layer__2023-10-20_18-52-28/lightning_logs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m   | Name | Type       | Params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m ------------------------------------[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m 0 | s1   | Sequential | 87.1 K[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m 87.1 K    Trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m 0         Non-trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m 87.1 K    Total params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m 0.349     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m   rank_zero_warn([32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_73c31454_63_batch_size=32,layer_1_size=123,layer_2_size=61,layer_3_size=32,layer_4_size=249,layer_5_size=249,layer_6__2023-10-20_18-52-44/checkpoint_000004)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ed70a45e_60_batch_size=128,layer_1_size=227,layer_2_size=32,layer_3_size=32,layer_4_size=169,layer_5_size=198,layer_6_2023-10-20_18-52-10/checkpoint_000031)[32m [repeated 39x across cluster][0m
[2m[36m(TorchTrainer pid=398446)[0m Starting distributed worker processes: ['399772 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=397942)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f6a500cf_61_batch_size=32,layer_1_size=32,layer_2_size=146,layer_3_size=249,layer_4_size=32,layer_5_size=32,layer_6_s_2023-10-20_18-52-19/checkpoint_000015)[32m [repeated 40x across cluster][0m
[2m[36m(RayTrainWorker pid=397587)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ed70a45e_60_batch_size=128,layer_1_size=227,layer_2_size=32,layer_3_size=32,layer_4_size=169,layer_5_size=198,layer_6_2023-10-20_18-52-10/checkpoint_000035)[32m [repeated 39x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000259)[32m [repeated 36x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001004)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=399772)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001005)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=396797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_06eddf3a_59_batch_size=256,layer_1_size=72,layer_2_size=92,layer_3_size=249,layer_4_size=149,layer_5_size=56,layer_6__2023-10-20_18-52-01/checkpoint_000058)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000123)[32m [repeated 24x across cluster][0m
[2m[36m(TrainTrainable pid=399951)[0m Trainable.setup took 14.697 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=399772)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=399772)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=399772)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=399772)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000278)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=399772)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fae66c94_64_batch_size=32,layer_1_size=32,layer_2_size=139,layer_3_size=151,layer_4_size=95,layer_5_size=157,layer_6__2023-10-20_18-52-48/lightning_logs
[2m[36m(RayTrainWorker pid=399772)[0m 
[2m[36m(RayTrainWorker pid=399772)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=399772)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=399772)[0m 0 | s1   | Sequential | 82.4 K
[2m[36m(RayTrainWorker pid=399772)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=399772)[0m 82.4 K    Trainable params
[2m[36m(RayTrainWorker pid=399772)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=399772)[0m 82.4 K    Total params
[2m[36m(RayTrainWorker pid=399772)[0m 0.330     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=399772)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=399772)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=399772)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=399772)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=399772)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=399772)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001017)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000241)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000135)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_73c31454_63_batch_size=32,layer_1_size=123,layer_2_size=61,layer_3_size=32,layer_4_size=249,layer_5_size=249,layer_6__2023-10-20_18-52-44/checkpoint_000046)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001027)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000247)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=398438)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3c14bfb3_62_batch_size=32,layer_1_size=249,layer_2_size=209,layer_3_size=249,layer_4_size=249,layer_5_size=130,layer__2023-10-20_18-52-28/checkpoint_000044)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=397942)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f6a500cf_61_batch_size=32,layer_1_size=32,layer_2_size=146,layer_3_size=249,layer_4_size=32,layer_5_size=32,layer_6_s_2023-10-20_18-52-19/checkpoint_000051)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=397942)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f6a500cf_61_batch_size=32,layer_1_size=32,layer_2_size=146,layer_3_size=249,layer_4_size=32,layer_5_size=32,layer_6_s_2023-10-20_18-52-19/checkpoint_000055)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000146)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=396797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_06eddf3a_59_batch_size=256,layer_1_size=72,layer_2_size=92,layer_3_size=249,layer_4_size=149,layer_5_size=56,layer_6__2023-10-20_18-52-01/checkpoint_000084)[32m [repeated 21x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000148)[32m [repeated 24x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_73c31454_63_batch_size=32,layer_1_size=123,layer_2_size=61,layer_3_size=32,layer_4_size=249,layer_5_size=249,layer_6__2023-10-20_18-52-44/checkpoint_000062)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=399772)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fae66c94_64_batch_size=32,layer_1_size=32,layer_2_size=139,layer_3_size=151,layer_4_size=95,layer_5_size=157,layer_6__2023-10-20_18-52-48/checkpoint_000015)[32m [repeated 21x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000154)[32m [repeated 32x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_73c31454_63_batch_size=32,layer_1_size=123,layer_2_size=61,layer_3_size=32,layer_4_size=249,layer_5_size=249,layer_6__2023-10-20_18-52-44/checkpoint_000077)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001047)[32m [repeated 30x across cluster][0m
[2m[36m(TorchTrainer pid=399951)[0m Starting distributed worker processes: ['400427 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000170)[32m [repeated 38x across cluster][0m
[2m[36m(RayTrainWorker pid=396762)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_cf41ad0c_58_batch_size=256,layer_1_size=144,layer_2_size=249,layer_3_size=158,layer_4_size=168,layer_5_size=74,layer__2023-10-20_18-51-11/checkpoint_000090)[32m [repeated 40x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_73c31454_63_batch_size=32,layer_1_size=123,layer_2_size=61,layer_3_size=32,layer_4_size=249,layer_5_size=249,layer_6__2023-10-20_18-52-44/checkpoint_000094)[32m [repeated 39x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000299)[32m [repeated 38x across cluster][0m
[2m[36m(RayTrainWorker pid=400427)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=397942)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f6a500cf_61_batch_size=32,layer_1_size=32,layer_2_size=146,layer_3_size=249,layer_4_size=32,layer_5_size=32,layer_6_s_2023-10-20_18-52-19/checkpoint_000093)[32m [repeated 40x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001064)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000190)[32m [repeated 45x across cluster][0m
[2m[36m(TrainTrainable pid=400428)[0m Trainable.setup took 16.445 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=400427)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=400427)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=400427)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=400427)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=400427)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_41b3dd8a_65_batch_size=256,layer_1_size=32,layer_2_size=126,layer_3_size=129,layer_4_size=74,layer_5_size=76,layer_6__2023-10-20_18-53-16/lightning_logs
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000195)[32m [repeated 37x across cluster][0m
[2m[36m(RayTrainWorker pid=400427)[0m 
[2m[36m(RayTrainWorker pid=400427)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=400427)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=400427)[0m 0 | s1   | Sequential | 55.9 K
[2m[36m(RayTrainWorker pid=400427)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=400427)[0m 55.9 K    Trainable params
[2m[36m(RayTrainWorker pid=400427)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=400427)[0m 55.9 K    Total params
[2m[36m(RayTrainWorker pid=400427)[0m 0.224     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=400427)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=400427)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=400427)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=400427)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=400427)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=400427)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000200)[32m [repeated 29x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['400843 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000343)[32m [repeated 31x across cluster][0m
[2m[36m(TorchTrainer pid=396257)[0m Starting distributed worker processes: ['400859 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000211)[32m [repeated 35x across cluster][0m
[2m[36m(RayTrainWorker pid=398438)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3c14bfb3_62_batch_size=32,layer_1_size=249,layer_2_size=209,layer_3_size=249,layer_4_size=249,layer_5_size=130,layer__2023-10-20_18-52-28/checkpoint_000097)[32m [repeated 64x across cluster][0m
[2m[36m(RayTrainWorker pid=400843)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000366)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=400859)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=400427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_41b3dd8a_65_batch_size=256,layer_1_size=32,layer_2_size=126,layer_3_size=129,layer_4_size=74,layer_5_size=76,layer_6__2023-10-20_18-53-16/checkpoint_000017)[32m [repeated 28x across cluster][0m
[2m[36m(TrainTrainable pid=400844)[0m Trainable.setup took 12.081 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=400843)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=400843)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=400843)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=400843)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=400843)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0e475eea_66_batch_size=64,layer_1_size=205,layer_2_size=141,layer_3_size=32,layer_4_size=249,layer_5_size=101,layer_6_2023-10-20_18-55-16/lightning_logs
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000221)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=400843)[0m 
[2m[36m(RayTrainWorker pid=400843)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=400843)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=400843)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=400843)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=400843)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=400843)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=400843)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=400843)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=400843)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=400843)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=400843)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=400843)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=400843)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=400843)[0m   rank_zero_warn(
[2m[36m(TrainTrainable pid=400860)[0m Trainable.setup took 12.291 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=400859)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=400859)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=400859)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=400859)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=400859)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_953b94d8_67_batch_size=64,layer_1_size=92,layer_2_size=70,layer_3_size=181,layer_4_size=32,layer_5_size=249,layer_6_s_2023-10-20_18-56-02/lightning_logs
[2m[36m(RayTrainWorker pid=397942)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f6a500cf_61_batch_size=32,layer_1_size=32,layer_2_size=146,layer_3_size=249,layer_4_size=32,layer_5_size=32,layer_6_s_2023-10-20_18-52-19/checkpoint_000135)[32m [repeated 32x across cluster][0m
[2m[36m(RayTrainWorker pid=400859)[0m 
[2m[36m(RayTrainWorker pid=400859)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=400859)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=400859)[0m 0 | s1   | Sequential | 196 K 
[2m[36m(RayTrainWorker pid=400859)[0m 196 K     Trainable params
[2m[36m(RayTrainWorker pid=400859)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=400859)[0m 196 K     Total params
[2m[36m(RayTrainWorker pid=400859)[0m 0.787     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=400859)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=400859)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=400859)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=400859)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001106)[32m [repeated 30x across cluster][0m
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['401767 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=400843)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0e475eea_66_batch_size=64,layer_1_size=205,layer_2_size=141,layer_3_size=32,layer_4_size=249,layer_5_size=101,layer_6_2023-10-20_18-55-16/checkpoint_000007)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=396797)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_06eddf3a_59_batch_size=256,layer_1_size=72,layer_2_size=92,layer_3_size=249,layer_4_size=149,layer_5_size=56,layer_6__2023-10-20_18-52-01/checkpoint_000154)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=400427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_41b3dd8a_65_batch_size=256,layer_1_size=32,layer_2_size=126,layer_3_size=129,layer_4_size=74,layer_5_size=76,layer_6__2023-10-20_18-53-16/checkpoint_000036)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=401767)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=400843)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0e475eea_66_batch_size=64,layer_1_size=205,layer_2_size=141,layer_3_size=32,layer_4_size=249,layer_5_size=101,layer_6_2023-10-20_18-55-16/checkpoint_000023)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001123)[32m [repeated 24x across cluster][0m
[2m[36m(TrainTrainable pid=401768)[0m Trainable.setup took 10.666 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=401767)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=401767)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=401767)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=401767)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=401767)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e1219613_68_batch_size=32,layer_1_size=166,layer_2_size=32,layer_3_size=158,layer_4_size=32,layer_5_size=145,layer_6__2023-10-20_18-56-10/lightning_logs
[2m[36m(RayTrainWorker pid=401767)[0m 
[2m[36m(RayTrainWorker pid=401767)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=401767)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=401767)[0m 0 | s1   | Sequential | 156 K 
[2m[36m(RayTrainWorker pid=401767)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=401767)[0m 156 K     Trainable params
[2m[36m(RayTrainWorker pid=401767)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=401767)[0m 156 K     Total params
[2m[36m(RayTrainWorker pid=401767)[0m 0.627     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=401767)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=401767)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=401767)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=401767)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=401767)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=401767)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=398445)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_73c31454_63_batch_size=32,layer_1_size=123,layer_2_size=61,layer_3_size=32,layer_4_size=249,layer_5_size=249,layer_6__2023-10-20_18-52-44/checkpoint_000149)[32m [repeated 24x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000392)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=400843)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0e475eea_66_batch_size=64,layer_1_size=205,layer_2_size=141,layer_3_size=32,layer_4_size=249,layer_5_size=101,layer_6_2023-10-20_18-55-16/checkpoint_000030)[32m [repeated 26x across cluster][0m
[2m[36m(TorchTrainer pid=401768)[0m Starting distributed worker processes: ['402427 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000403)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000241)[32m [repeated 31x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000381)[32m [repeated 35x across cluster][0m
[2m[36m(TorchTrainer pid=398446)[0m Starting distributed worker processes: ['402859 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=402427)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000409)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=383592)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4dd741f8_33_batch_size=128,layer_1_size=54,layer_2_size=198,layer_3_size=173,layer_4_size=209,layer_5_size=125,layer__2023-10-20_18-43-34/checkpoint_001136)[32m [repeated 23x across cluster][0m
[2m[36m(TrainTrainable pid=402428)[0m Trainable.setup took 11.074 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=402427)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=402427)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=402427)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=402427)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=402859)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=402427)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2bd018e3_69_batch_size=32,layer_1_size=145,layer_2_size=107,layer_3_size=197,layer_4_size=141,layer_5_size=135,layer__2023-10-20_18-56-48/lightning_logs
[2m[36m(RayTrainWorker pid=400859)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_953b94d8_67_batch_size=64,layer_1_size=92,layer_2_size=70,layer_3_size=181,layer_4_size=32,layer_5_size=249,layer_6_s_2023-10-20_18-56-02/checkpoint_000040)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=402427)[0m 
[2m[36m(RayTrainWorker pid=402427)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=402427)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=402427)[0m 0 | s1   | Sequential | 122 K 
[2m[36m(RayTrainWorker pid=402427)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=402427)[0m 122 K     Trainable params
[2m[36m(RayTrainWorker pid=402427)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=402427)[0m 122 K     Total params
[2m[36m(RayTrainWorker pid=402427)[0m 0.489     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=402427)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=402427)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=402427)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=402427)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=402427)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=402427)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=402859)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=402859)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=402859)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=402859)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=402859)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17222e8c_70_batch_size=64,layer_1_size=249,layer_2_size=249,layer_3_size=189,layer_4_size=135,layer_5_size=153,layer__2023-10-20_18-57-28/lightning_logs
[2m[36m(RayTrainWorker pid=402427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2bd018e3_69_batch_size=32,layer_1_size=145,layer_2_size=107,layer_3_size=197,layer_4_size=141,layer_5_size=135,layer__2023-10-20_18-56-48/checkpoint_000002)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=402859)[0m 
[2m[36m(RayTrainWorker pid=402859)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=402859)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=402859)[0m 0 | s1   | Sequential | 82.4 K
[2m[36m(RayTrainWorker pid=402859)[0m 82.4 K    Trainable params
[2m[36m(RayTrainWorker pid=402859)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=402859)[0m 82.4 K    Total params
[2m[36m(RayTrainWorker pid=402859)[0m 0.330     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=402859)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=402859)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=402859)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=402859)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=402427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2bd018e3_69_batch_size=32,layer_1_size=145,layer_2_size=107,layer_3_size=197,layer_4_size=141,layer_5_size=135,layer__2023-10-20_18-56-48/checkpoint_000006)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_73c31454_63_batch_size=32,layer_1_size=123,layer_2_size=61,layer_3_size=32,layer_4_size=249,layer_5_size=249,layer_6__2023-10-20_18-52-44/checkpoint_000168)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=400843)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0e475eea_66_batch_size=64,layer_1_size=205,layer_2_size=141,layer_3_size=32,layer_4_size=249,layer_5_size=101,layer_6_2023-10-20_18-55-16/checkpoint_000039)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=394179)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7425c5d6_54_batch_size=128,layer_1_size=32,layer_2_size=135,layer_3_size=106,layer_4_size=249,layer_5_size=32,layer_6_2023-10-20_18-49-51/checkpoint_000397)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000419)[32m [repeated 49x across cluster][0m
[2m[36m(RayTrainWorker pid=402859)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17222e8c_70_batch_size=64,layer_1_size=249,layer_2_size=249,layer_3_size=189,layer_4_size=135,layer_5_size=153,layer__2023-10-20_18-57-28/checkpoint_000014)[32m [repeated 49x across cluster][0m
[2m[36m(RayTrainWorker pid=400843)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0e475eea_66_batch_size=64,layer_1_size=205,layer_2_size=141,layer_3_size=32,layer_4_size=249,layer_5_size=101,layer_6_2023-10-20_18-55-16/checkpoint_000053)[32m [repeated 57x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000266)[32m [repeated 53x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['403490 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000272)[32m [repeated 49x across cluster][0m
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['404308 (192.0.0.1)'][32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=398445)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_73c31454_63_batch_size=32,layer_1_size=123,layer_2_size=61,layer_3_size=32,layer_4_size=249,layer_5_size=249,layer_6__2023-10-20_18-52-44/checkpoint_000197)[32m [repeated 54x across cluster][0m
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['404563 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=400859)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_953b94d8_67_batch_size=64,layer_1_size=92,layer_2_size=70,layer_3_size=181,layer_4_size=32,layer_5_size=249,layer_6_s_2023-10-20_18-56-02/checkpoint_000071)[32m [repeated 51x across cluster][0m
[2m[36m(RayTrainWorker pid=402427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2bd018e3_69_batch_size=32,layer_1_size=145,layer_2_size=107,layer_3_size=197,layer_4_size=141,layer_5_size=135,layer__2023-10-20_18-56-48/checkpoint_000052)[32m [repeated 53x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=400843)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0e475eea_66_batch_size=64,layer_1_size=205,layer_2_size=141,layer_3_size=32,layer_4_size=249,layer_5_size=101,layer_6_2023-10-20_18-55-16/checkpoint_000085)[32m [repeated 57x across cluster][0m
[2m[36m(RayTrainWorker pid=404308)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 3x across cluster][0m
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['404994 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=402427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2bd018e3_69_batch_size=32,layer_1_size=145,layer_2_size=107,layer_3_size=197,layer_4_size=141,layer_5_size=135,layer__2023-10-20_18-56-48/checkpoint_000067)[32m [repeated 50x across cluster][0m
[2m[36m(RayTrainWorker pid=404563)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000301)[32m [repeated 48x across cluster][0m
[2m[36m(TrainTrainable pid=403467)[0m Trainable.setup took 13.909 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=403460)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=403460)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=403460)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=403460)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=403460)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/lightning_logs
[2m[36m(RayTrainWorker pid=403460)[0m 
[2m[36m(RayTrainWorker pid=403460)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=403460)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=403460)[0m 0 | s1   | Sequential | 186 K 
[2m[36m(RayTrainWorker pid=403460)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=403460)[0m 186 K     Trainable params
[2m[36m(RayTrainWorker pid=403460)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=403460)[0m 186 K     Total params
[2m[36m(RayTrainWorker pid=403460)[0m 0.746     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=403460)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=403460)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=404308)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=404308)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['405257 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=402427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2bd018e3_69_batch_size=32,layer_1_size=145,layer_2_size=107,layer_3_size=197,layer_4_size=141,layer_5_size=135,layer__2023-10-20_18-56-48/checkpoint_000083)[32m [repeated 42x across cluster][0m
[2m[36m(TrainTrainable pid=403461)[0m Trainable.setup took 13.917 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=403465)[0m GPU available: False, used: False[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m TPU available: False, using: 0 TPU cores[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m IPU available: False, using: 0 IPUs[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m HPU available: False, using: 0 HPUs[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=404994)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=403465)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7d4e6b9a_72_batch_size=32,layer_1_size=32,layer_2_size=210,layer_3_size=156,layer_4_size=101,layer_5_size=122,layer_6_2023-10-20_18-58-41/lightning_logs[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m [32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m   | Name | Type       | Params[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m ------------------------------------[32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m 0 | s1   | Sequential | 99.7 K[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m 99.7 K    Trainable params[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m 0         Non-trainable params[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m 99.7 K    Total params[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m 0.399     Total estimated model params size (MB)[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m   rank_zero_warn([32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=404308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b66ec02e_74_batch_size=32,layer_1_size=249,layer_2_size=249,layer_3_size=168,layer_4_size=249,layer_5_size=126,layer__2023-10-20_18-58-48/checkpoint_000007)[32m [repeated 56x across cluster][0m
[2m[36m(TorchTrainer pid=396257)[0m Starting distributed worker processes: ['405576 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=404994)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=404994)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=404994)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=404994)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=402859)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17222e8c_70_batch_size=64,layer_1_size=249,layer_2_size=249,layer_3_size=189,layer_4_size=135,layer_5_size=153,layer__2023-10-20_18-57-28/checkpoint_000071)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=404994)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/lightning_logs
[2m[36m(RayTrainWorker pid=405257)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=404994)[0m 
[2m[36m(RayTrainWorker pid=404994)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=404994)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=404994)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=404994)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=404994)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=404994)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=404994)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=404994)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=404994)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=404994)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=404994)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=404994)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=404994)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=404994)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000016)[32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=405257)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=405257)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=405257)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=405257)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=402859)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17222e8c_70_batch_size=64,layer_1_size=249,layer_2_size=249,layer_3_size=189,layer_4_size=135,layer_5_size=153,layer__2023-10-20_18-57-28/checkpoint_000076)[32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7e36c5a8_78_batch_size=64,layer_1_size=114,layer_2_size=249,layer_3_size=127,layer_4_size=248,layer_5_size=249,layer__2023-10-20_18-59-29/lightning_logs
[2m[36m(RayTrainWorker pid=405257)[0m 
[2m[36m(RayTrainWorker pid=405257)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=405257)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=405257)[0m 0 | s1   | Sequential | 156 K 
[2m[36m(RayTrainWorker pid=405257)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=405257)[0m 156 K     Trainable params
[2m[36m(RayTrainWorker pid=405257)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=405257)[0m 156 K     Total params
[2m[36m(RayTrainWorker pid=405257)[0m 0.627     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=405257)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=405257)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=405257)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=405257)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['406313 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=405576)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=405576)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=405576)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=405576)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000514)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=405257)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9bbcb5ff_77_batch_size=64,layer_1_size=143,layer_2_size=101,layer_3_size=134,layer_4_size=249,layer_5_size=249,layer__2023-10-20_18-59-15/lightning_logs
[2m[36m(RayTrainWorker pid=405576)[0m 
[2m[36m(RayTrainWorker pid=405576)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=405576)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m 0 | s1   | Sequential | 196 K 
[2m[36m(RayTrainWorker pid=405576)[0m 196 K     Trainable params
[2m[36m(RayTrainWorker pid=405576)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=405576)[0m 196 K     Total params
[2m[36m(RayTrainWorker pid=405576)[0m 0.787     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=405576)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=405576)[0m   rank_zero_warn([32m [repeated 5x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=405576)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=402859)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17222e8c_70_batch_size=64,layer_1_size=249,layer_2_size=249,layer_3_size=189,layer_4_size=135,layer_5_size=153,layer__2023-10-20_18-57-28/checkpoint_000079)[32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7d4e6b9a_72_batch_size=32,layer_1_size=32,layer_2_size=210,layer_3_size=156,layer_4_size=101,layer_5_size=122,layer_6_2023-10-20_18-58-41/checkpoint_000009)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=406313)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=404308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b66ec02e_74_batch_size=32,layer_1_size=249,layer_2_size=249,layer_3_size=168,layer_4_size=249,layer_5_size=126,layer__2023-10-20_18-58-48/checkpoint_000013)[32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000011)[32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=402859)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17222e8c_70_batch_size=64,layer_1_size=249,layer_2_size=249,layer_3_size=189,layer_4_size=135,layer_5_size=153,layer__2023-10-20_18-57-28/checkpoint_000083)[32m [repeated 10x across cluster][0m
[2m[36m(TrainTrainable pid=406384)[0m Trainable.setup took 14.753 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=406313)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=406313)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=406313)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=406313)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=406313)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d35953af_79_batch_size=256,layer_1_size=43,layer_2_size=249,layer_3_size=249,layer_4_size=53,layer_5_size=249,layer_6_2023-10-20_18-59-37/lightning_logs
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000005)[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=406313)[0m 
[2m[36m(RayTrainWorker pid=406313)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=406313)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=406313)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=406313)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=406313)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=406313)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=406313)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=406313)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=406313)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=406313)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=406313)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=406313)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=406313)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=406313)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=405576)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7e36c5a8_78_batch_size=64,layer_1_size=114,layer_2_size=249,layer_3_size=127,layer_4_size=248,layer_5_size=249,layer__2023-10-20_18-59-29/checkpoint_000006)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=405257)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9bbcb5ff_77_batch_size=64,layer_1_size=143,layer_2_size=101,layer_3_size=134,layer_4_size=249,layer_5_size=249,layer__2023-10-20_18-59-15/checkpoint_000008)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000011)[32m [repeated 17x across cluster][0m
[2m[36m(TorchTrainer pid=406384)[0m Starting distributed worker processes: ['406996 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=400427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_41b3dd8a_65_batch_size=256,layer_1_size=32,layer_2_size=126,layer_3_size=129,layer_4_size=74,layer_5_size=76,layer_6__2023-10-20_18-53-16/checkpoint_000173)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000015)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000016)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000019)[32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=404563)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81c6d93f_75_batch_size=256,layer_1_size=171,layer_2_size=90,layer_3_size=249,layer_4_size=249,layer_5_size=167,layer__2023-10-20_18-58-57/checkpoint_000023)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=402427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2bd018e3_69_batch_size=32,layer_1_size=145,layer_2_size=107,layer_3_size=197,layer_4_size=141,layer_5_size=135,layer__2023-10-20_18-56-48/checkpoint_000117)[32m [repeated 9x across cluster][0m
[2m[36m(TrainTrainable pid=407121)[0m Trainable.setup took 13.104 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(RayTrainWorker pid=406996)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=406996)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=406996)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=406996)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=406996)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/lightning_logs
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000021)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m 
[2m[36m(RayTrainWorker pid=406996)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=406996)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=406996)[0m 0 | s1   | Sequential | 138 K 
[2m[36m(RayTrainWorker pid=406996)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=406996)[0m 138 K     Trainable params
[2m[36m(RayTrainWorker pid=406996)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=406996)[0m 138 K     Total params
[2m[36m(RayTrainWorker pid=406996)[0m 0.553     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=406996)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=406996)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=406996)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=406996)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=406996)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=406996)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000034)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=402859)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_17222e8c_70_batch_size=64,layer_1_size=249,layer_2_size=249,layer_3_size=189,layer_4_size=135,layer_5_size=153,layer__2023-10-20_18-57-28/checkpoint_000099)[32m [repeated 35x across cluster][0m
[2m[36m(TorchTrainer pid=407121)[0m Starting distributed worker processes: ['407614 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=404308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b66ec02e_74_batch_size=32,layer_1_size=249,layer_2_size=249,layer_3_size=168,layer_4_size=249,layer_5_size=126,layer__2023-10-20_18-58-48/checkpoint_000036)[32m [repeated 46x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7e36c5a8_78_batch_size=64,layer_1_size=114,layer_2_size=249,layer_3_size=127,layer_4_size=248,layer_5_size=249,layer__2023-10-20_18-59-29/checkpoint_000022)[32m [repeated 44x across cluster][0m
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000037)[32m [repeated 46x across cluster][0m
[2m[36m(RayTrainWorker pid=404563)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81c6d93f_75_batch_size=256,layer_1_size=171,layer_2_size=90,layer_3_size=249,layer_4_size=249,layer_5_size=167,layer__2023-10-20_18-58-57/checkpoint_000046)[32m [repeated 50x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7e36c5a8_78_batch_size=64,layer_1_size=114,layer_2_size=249,layer_3_size=127,layer_4_size=248,layer_5_size=249,layer__2023-10-20_18-59-29/checkpoint_000040)[32m [repeated 29x across cluster][0m
[2m[36m(TorchTrainer pid=398446)[0m Starting distributed worker processes: ['407873 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=407614)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=405257)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9bbcb5ff_77_batch_size=64,layer_1_size=143,layer_2_size=101,layer_3_size=134,layer_4_size=249,layer_5_size=249,layer__2023-10-20_18-59-15/checkpoint_000038)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000045)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=407614)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=407614)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=407614)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=407873)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=407614)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/lightning_logs
[2m[36m(RayTrainWorker pid=407614)[0m 
[2m[36m(RayTrainWorker pid=407614)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=407614)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=407614)[0m 0 | s1   | Sequential | 227 K 
[2m[36m(RayTrainWorker pid=407614)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=407614)[0m 227 K     Trainable params
[2m[36m(RayTrainWorker pid=407614)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=407614)[0m 227 K     Total params
[2m[36m(RayTrainWorker pid=407614)[0m 0.909     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=407614)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=407614)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=407614)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=407614)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=407614)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=407614)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000048)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=407873)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=407873)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=407873)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=407873)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=407873)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba1dff5f_82_batch_size=32,layer_1_size=197,layer_2_size=43,layer_3_size=138,layer_4_size=137,layer_5_size=103,layer_6_2023-10-20_19-01-29/lightning_logs
[2m[36m(RayTrainWorker pid=407873)[0m 
[2m[36m(RayTrainWorker pid=407873)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=407873)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=407873)[0m 0 | s1   | Sequential | 82.4 K
[2m[36m(RayTrainWorker pid=407873)[0m 82.4 K    Trainable params
[2m[36m(RayTrainWorker pid=407873)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=407873)[0m 82.4 K    Total params
[2m[36m(RayTrainWorker pid=407873)[0m 0.330     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=407873)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=407873)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=407873)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=407873)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=405576)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7e36c5a8_78_batch_size=64,layer_1_size=114,layer_2_size=249,layer_3_size=127,layer_4_size=248,layer_5_size=249,layer__2023-10-20_18-59-29/checkpoint_000042)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000552)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000055)[32m [repeated 10x across cluster][0m
[2m[36m(RayTrainWorker pid=407873)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba1dff5f_82_batch_size=32,layer_1_size=197,layer_2_size=43,layer_3_size=138,layer_4_size=137,layer_5_size=103,layer_6_2023-10-20_19-01-29/checkpoint_000004)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=402427)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2bd018e3_69_batch_size=32,layer_1_size=145,layer_2_size=107,layer_3_size=197,layer_4_size=141,layer_5_size=135,layer__2023-10-20_18-56-48/checkpoint_000135)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=404563)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81c6d93f_75_batch_size=256,layer_1_size=171,layer_2_size=90,layer_3_size=249,layer_4_size=249,layer_5_size=167,layer__2023-10-20_18-58-57/checkpoint_000054)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000053)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000045)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7d4e6b9a_72_batch_size=32,layer_1_size=32,layer_2_size=210,layer_3_size=156,layer_4_size=101,layer_5_size=122,layer_6_2023-10-20_18-58-41/checkpoint_000056)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7e36c5a8_78_batch_size=64,layer_1_size=114,layer_2_size=249,layer_3_size=127,layer_4_size=248,layer_5_size=249,layer__2023-10-20_18-59-29/checkpoint_000048)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=404563)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_81c6d93f_75_batch_size=256,layer_1_size=171,layer_2_size=90,layer_3_size=249,layer_4_size=249,layer_5_size=167,layer__2023-10-20_18-58-57/checkpoint_000061)[32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=406313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d35953af_79_batch_size=256,layer_1_size=43,layer_2_size=249,layer_3_size=249,layer_4_size=53,layer_5_size=249,layer_6_2023-10-20_18-59-37/checkpoint_000033)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=404308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b66ec02e_74_batch_size=32,layer_1_size=249,layer_2_size=249,layer_3_size=168,layer_4_size=249,layer_5_size=126,layer__2023-10-20_18-58-48/checkpoint_000071)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7e36c5a8_78_batch_size=64,layer_1_size=114,layer_2_size=249,layer_3_size=127,layer_4_size=248,layer_5_size=249,layer__2023-10-20_18-59-29/checkpoint_000051)[32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000361)[32m [repeated 13x across cluster][0m
[2m[36m(TorchTrainer pid=399951)[0m Starting distributed worker processes: ['408168 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=403465)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7d4e6b9a_72_batch_size=32,layer_1_size=32,layer_2_size=210,layer_3_size=156,layer_4_size=101,layer_5_size=122,layer_6_2023-10-20_18-58-41/checkpoint_000066)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000019)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000064)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=407873)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba1dff5f_82_batch_size=32,layer_1_size=197,layer_2_size=43,layer_3_size=138,layer_4_size=137,layer_5_size=103,layer_6_2023-10-20_19-01-29/checkpoint_000017)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000054)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=408168)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=404994)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ea247724_76_batch_size=128,layer_1_size=32,layer_2_size=148,layer_3_size=249,layer_4_size=144,layer_5_size=234,layer__2023-10-20_18-59-02/checkpoint_000068)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000023)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000057)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=408168)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=408168)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=408168)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=408168)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=408168)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f702b070_83_batch_size=256,layer_1_size=195,layer_2_size=203,layer_3_size=186,layer_4_size=123,layer_5_size=128,layer_2023-10-20_19-01-47/lightning_logs
[2m[36m(RayTrainWorker pid=403465)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7d4e6b9a_72_batch_size=32,layer_1_size=32,layer_2_size=210,layer_3_size=156,layer_4_size=101,layer_5_size=122,layer_6_2023-10-20_18-58-41/checkpoint_000075)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=408168)[0m 
[2m[36m(RayTrainWorker pid=408168)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=408168)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=408168)[0m 0 | s1   | Sequential | 55.9 K
[2m[36m(RayTrainWorker pid=408168)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=408168)[0m 55.9 K    Trainable params
[2m[36m(RayTrainWorker pid=408168)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=408168)[0m 55.9 K    Total params
[2m[36m(RayTrainWorker pid=408168)[0m 0.224     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=408168)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=408168)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=408168)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=408168)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=408168)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=408168)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000574)[32m [repeated 35x across cluster][0m
[2m[36m(RayTrainWorker pid=404308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b66ec02e_74_batch_size=32,layer_1_size=249,layer_2_size=249,layer_3_size=168,layer_4_size=249,layer_5_size=126,layer__2023-10-20_18-58-48/checkpoint_000089)[32m [repeated 32x across cluster][0m
[2m[36m(RayTrainWorker pid=403465)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7d4e6b9a_72_batch_size=32,layer_1_size=32,layer_2_size=210,layer_3_size=156,layer_4_size=101,layer_5_size=122,layer_6_2023-10-20_18-58-41/checkpoint_000083)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000035)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000050)[32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000084)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=405257)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9bbcb5ff_77_batch_size=64,layer_1_size=143,layer_2_size=101,layer_3_size=134,layer_4_size=249,layer_5_size=249,layer__2023-10-20_18-59-15/checkpoint_000085)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000058)[32m [repeated 49x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000392)[32m [repeated 48x across cluster][0m
[2m[36m(RayTrainWorker pid=404308)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b66ec02e_74_batch_size=32,layer_1_size=249,layer_2_size=249,layer_3_size=168,layer_4_size=249,layer_5_size=126,layer__2023-10-20_18-58-48/checkpoint_000100)[32m [repeated 38x across cluster][0m
[2m[36m(RayTrainWorker pid=405576)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_7e36c5a8_78_batch_size=64,layer_1_size=114,layer_2_size=249,layer_3_size=127,layer_4_size=248,layer_5_size=249,layer__2023-10-20_18-59-29/checkpoint_000083)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000068)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000080)[32m [repeated 30x across cluster][0m
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['408440 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000076)[32m [repeated 29x across cluster][0m
[2m[36m(TorchTrainer pid=383605)[0m Starting distributed worker processes: ['408437 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000082)[32m [repeated 28x across cluster][0m
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['408913 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=407873)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba1dff5f_82_batch_size=32,layer_1_size=197,layer_2_size=43,layer_3_size=138,layer_4_size=137,layer_5_size=103,layer_6_2023-10-20_19-01-29/checkpoint_000074)[32m [repeated 10x across cluster][0m
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000088)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=408437)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=403490)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c45e4b_73_batch_size=32,layer_1_size=169,layer_2_size=157,layer_3_size=249,layer_4_size=138,layer_5_size=249,layer__2023-10-20_18-58-46/checkpoint_000092)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=408440)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=408913)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=407873)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba1dff5f_82_batch_size=32,layer_1_size=197,layer_2_size=43,layer_3_size=138,layer_4_size=137,layer_5_size=103,layer_6_2023-10-20_19-01-29/checkpoint_000082)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=408437)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=408437)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=408437)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=408437)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000092)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=408437)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc93d21a_84_batch_size=64,layer_1_size=51,layer_2_size=114,layer_3_size=32,layer_4_size=151,layer_5_size=249,layer_6__2023-10-20_19-03-21/lightning_logs
[2m[36m(RayTrainWorker pid=408437)[0m 
[2m[36m(RayTrainWorker pid=408437)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=408437)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=408437)[0m 0 | s1   | Sequential | 99.7 K
[2m[36m(RayTrainWorker pid=408437)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=408437)[0m 99.7 K    Trainable params
[2m[36m(RayTrainWorker pid=408437)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=408437)[0m 99.7 K    Total params
[2m[36m(RayTrainWorker pid=408437)[0m 0.399     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=408437)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=408437)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=408437)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=408437)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=408437)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=408437)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=408913)[0m GPU available: False, used: False[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m TPU available: False, using: 0 TPU cores[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m IPU available: False, using: 0 IPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m HPU available: False, using: 0 HPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408440)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3880fd2f_85_batch_size=256,layer_1_size=114,layer_2_size=110,layer_3_size=137,layer_4_size=139,layer_5_size=99,layer__2023-10-20_19-05-14/checkpoint_000000)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_53863394_86_batch_size=256,layer_1_size=149,layer_2_size=66,layer_3_size=249,layer_4_size=32,layer_5_size=149,layer_6_2023-10-20_19-05-18/lightning_logs[32m [repeated 2x across cluster][0m
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['409167 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=408913)[0m [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m   | Name | Type       | Params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m ------------------------------------[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m 0 | s1   | Sequential | 156 K [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m 156 K     Trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m 0         Non-trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m 156 K     Total params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m 0.627     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m   rank_zero_warn([32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000067)[32m [repeated 31x across cluster][0m
[2m[36m(TorchTrainer pid=396257)[0m Starting distributed worker processes: ['409267 (192.0.0.1)'][32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408437)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc93d21a_84_batch_size=64,layer_1_size=51,layer_2_size=114,layer_3_size=32,layer_4_size=151,layer_5_size=249,layer_6__2023-10-20_19-03-21/checkpoint_000007)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000101)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=409173)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['409955 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=406313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d35953af_79_batch_size=256,layer_1_size=43,layer_2_size=249,layer_3_size=249,layer_4_size=53,layer_5_size=249,layer_6_2023-10-20_18-59-37/checkpoint_000086)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 2x across cluster][0m
[2m[36m(TorchTrainer pid=401768)[0m Starting distributed worker processes: ['409962 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=409173)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=409173)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=409173)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=409173)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=409173)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3e3153e0_88_batch_size=32,layer_1_size=180,layer_2_size=249,layer_3_size=101,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-06-04/lightning_logs
[2m[36m(RayTrainWorker pid=408437)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc93d21a_84_batch_size=64,layer_1_size=51,layer_2_size=114,layer_3_size=32,layer_4_size=151,layer_5_size=249,layer_6__2023-10-20_19-03-21/checkpoint_000010)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=409173)[0m 
[2m[36m(RayTrainWorker pid=409173)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=409173)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=409173)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=409173)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=409173)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=409173)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=409173)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=409173)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=409173)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=409173)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=409173)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=409173)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=409173)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=409173)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=409955)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=409267)[0m GPU available: False, used: False[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m TPU available: False, using: 0 TPU cores[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m IPU available: False, using: 0 IPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m HPU available: False, using: 0 HPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409167)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e280f659_87_batch_size=256,layer_1_size=206,layer_2_size=249,layer_3_size=131,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-05-36/lightning_logs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409167)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e280f659_87_batch_size=256,layer_1_size=206,layer_2_size=249,layer_3_size=131,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-05-36/checkpoint_000002)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m   | Name | Type       | Params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m ------------------------------------[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m 0 | s1   | Sequential | 196 K [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m 196 K     Trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m 0         Non-trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m 196 K     Total params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m 0.787     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m   rank_zero_warn([32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=408437)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc93d21a_84_batch_size=64,layer_1_size=51,layer_2_size=114,layer_3_size=32,layer_4_size=151,layer_5_size=249,layer_6__2023-10-20_19-03-21/checkpoint_000015)[32m [repeated 32x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=409962)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=409962)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=409962)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=409962)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/lightning_logs
[2m[36m(RayTrainWorker pid=409955)[0m 
[2m[36m(RayTrainWorker pid=409955)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=409955)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=409955)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=409955)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=409955)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=409955)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=409955)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=409955)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=409955)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=409955)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=409962)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=409962)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=407873)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba1dff5f_82_batch_size=32,layer_1_size=197,layer_2_size=43,layer_3_size=138,layer_4_size=137,layer_5_size=103,layer_6_2023-10-20_19-01-29/checkpoint_000098)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=409955)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=409955)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=409955)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=409955)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=409955)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/lightning_logs
[2m[36m(RayTrainWorker pid=409962)[0m 
[2m[36m(RayTrainWorker pid=409962)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=409962)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m 0 | s1   | Sequential | 122 K 
[2m[36m(RayTrainWorker pid=409962)[0m 122 K     Trainable params
[2m[36m(RayTrainWorker pid=409962)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=409962)[0m 122 K     Total params
[2m[36m(RayTrainWorker pid=409962)[0m 0.489     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=409962)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=409955)[0m   rank_zero_warn([32m [repeated 5x across cluster][0m
[2m[36m(RayTrainWorker pid=409955)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=409955)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=406313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d35953af_79_batch_size=256,layer_1_size=43,layer_2_size=249,layer_3_size=249,layer_4_size=53,layer_5_size=249,layer_6_2023-10-20_18-59-37/checkpoint_000093)[32m [repeated 35x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000633)[32m [repeated 38x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000008)[32m [repeated 44x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_53863394_86_batch_size=256,layer_1_size=149,layer_2_size=66,layer_3_size=249,layer_4_size=32,layer_5_size=149,layer_6_2023-10-20_19-05-18/checkpoint_000027)[32m [repeated 40x across cluster][0m
[2m[36m(RayTrainWorker pid=409955)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/checkpoint_000020)[32m [repeated 50x across cluster][0m
[2m[36m(TorchTrainer pid=398446)[0m Starting distributed worker processes: ['410574 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000441)[32m [repeated 38x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000132)[32m [repeated 44x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000106)[32m [repeated 43x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_53863394_86_batch_size=256,layer_1_size=149,layer_2_size=66,layer_3_size=249,layer_4_size=32,layer_5_size=149,layer_6_2023-10-20_19-05-18/checkpoint_000043)[32m [repeated 45x across cluster][0m
[2m[36m(RayTrainWorker pid=410574)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=406313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d35953af_79_batch_size=256,layer_1_size=43,layer_2_size=249,layer_3_size=249,layer_4_size=53,layer_5_size=249,layer_6_2023-10-20_18-59-37/checkpoint_000125)[32m [repeated 44x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3fd90e5c_89_batch_size=256,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=79,layer_5_size=32,layer_6__2023-10-20_19-06-07/checkpoint_000027)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=406313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d35953af_79_batch_size=256,layer_1_size=43,layer_2_size=249,layer_3_size=249,layer_4_size=53,layer_5_size=249,layer_6_2023-10-20_18-59-37/checkpoint_000131)[32m [repeated 37x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000452)[32m [repeated 43x across cluster][0m
[2m[36m(RayTrainWorker pid=410574)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=410574)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=410574)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=410574)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=410574)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/lightning_logs
[2m[36m(RayTrainWorker pid=409955)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/checkpoint_000048)[32m [repeated 41x across cluster][0m
[2m[36m(RayTrainWorker pid=410574)[0m 
[2m[36m(RayTrainWorker pid=410574)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=410574)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=410574)[0m 0 | s1   | Sequential | 82.4 K
[2m[36m(RayTrainWorker pid=410574)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=410574)[0m 82.4 K    Trainable params
[2m[36m(RayTrainWorker pid=410574)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=410574)[0m 82.4 K    Total params
[2m[36m(RayTrainWorker pid=410574)[0m 0.330     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=410574)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=410574)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=410574)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=410574)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=410574)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=410574)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=409173)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3e3153e0_88_batch_size=32,layer_1_size=180,layer_2_size=249,layer_3_size=101,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-06-04/checkpoint_000045)[32m [repeated 50x across cluster][0m
[2m[36m(RayTrainWorker pid=408168)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f702b070_83_batch_size=256,layer_1_size=195,layer_2_size=203,layer_3_size=186,layer_4_size=123,layer_5_size=128,layer_2023-10-20_19-01-47/checkpoint_000093)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000686)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=409167)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e280f659_87_batch_size=256,layer_1_size=206,layer_2_size=249,layer_3_size=131,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-05-36/checkpoint_000047)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000139)[32m [repeated 40x across cluster][0m
[2m[36m(RayTrainWorker pid=409173)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3e3153e0_88_batch_size=32,layer_1_size=180,layer_2_size=249,layer_3_size=101,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-06-04/checkpoint_000057)[32m [repeated 45x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000467)[32m [repeated 41x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_53863394_86_batch_size=256,layer_1_size=149,layer_2_size=66,layer_3_size=249,layer_4_size=32,layer_5_size=149,layer_6_2023-10-20_19-05-18/checkpoint_000072)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000470)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3fd90e5c_89_batch_size=256,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=79,layer_5_size=32,layer_6__2023-10-20_19-06-07/checkpoint_000054)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000170)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=408440)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3880fd2f_85_batch_size=256,layer_1_size=114,layer_2_size=110,layer_3_size=137,layer_4_size=139,layer_5_size=99,layer__2023-10-20_19-05-14/checkpoint_000069)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=408913)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_53863394_86_batch_size=256,layer_1_size=149,layer_2_size=66,layer_3_size=249,layer_4_size=32,layer_5_size=149,layer_6_2023-10-20_19-05-18/checkpoint_000078)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000155)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=406313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d35953af_79_batch_size=256,layer_1_size=43,layer_2_size=249,layer_3_size=249,layer_4_size=53,layer_5_size=249,layer_6_2023-10-20_18-59-37/checkpoint_000150)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000705)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000180)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=408440)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3880fd2f_85_batch_size=256,layer_1_size=114,layer_2_size=110,layer_3_size=137,layer_4_size=139,layer_5_size=99,layer__2023-10-20_19-05-14/checkpoint_000075)[32m [repeated 32x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000190)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=410574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/checkpoint_000031)[32m [repeated 36x across cluster][0m
[2m[36m(RayTrainWorker pid=409267)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3fd90e5c_89_batch_size=256,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=79,layer_5_size=32,layer_6__2023-10-20_19-06-07/checkpoint_000066)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000194)[32m [repeated 10x across cluster][0m
[2m[36m(RayTrainWorker pid=408440)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3880fd2f_85_batch_size=256,layer_1_size=114,layer_2_size=110,layer_3_size=137,layer_4_size=139,layer_5_size=99,layer__2023-10-20_19-05-14/checkpoint_000086)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000717)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=410574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/checkpoint_000036)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=406313)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d35953af_79_batch_size=256,layer_1_size=43,layer_2_size=249,layer_3_size=249,layer_4_size=53,layer_5_size=249,layer_6_2023-10-20_18-59-37/checkpoint_000164)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=408168)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f702b070_83_batch_size=256,layer_1_size=195,layer_2_size=203,layer_3_size=186,layer_4_size=123,layer_5_size=128,layer_2023-10-20_19-01-47/checkpoint_000125)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=406996)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4b67aacc_80_batch_size=32,layer_1_size=157,layer_2_size=126,layer_3_size=131,layer_4_size=151,layer_5_size=137,layer__2023-10-20_18-59-51/checkpoint_000199)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=408440)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3880fd2f_85_batch_size=256,layer_1_size=114,layer_2_size=110,layer_3_size=137,layer_4_size=139,layer_5_size=99,layer__2023-10-20_19-05-14/checkpoint_000093)[32m [repeated 22x across cluster][0m
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['410926 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=409167)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e280f659_87_batch_size=256,layer_1_size=206,layer_2_size=249,layer_3_size=131,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-05-36/checkpoint_000084)[32m [repeated 24x across cluster][0m
[2m[36m(RayTrainWorker pid=409167)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e280f659_87_batch_size=256,layer_1_size=206,layer_2_size=249,layer_3_size=131,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-05-36/checkpoint_000085)[32m [repeated 24x across cluster][0m
[2m[36m(RayTrainWorker pid=409173)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3e3153e0_88_batch_size=32,layer_1_size=180,layer_2_size=249,layer_3_size=101,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-06-04/checkpoint_000084)[32m [repeated 22x across cluster][0m
[2m[36m(TorchTrainer pid=406384)[0m Starting distributed worker processes: ['411164 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000183)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=410926)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000088)[32m [repeated 24x across cluster][0m
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['411463 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=411164)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000500)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=409173)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3e3153e0_88_batch_size=32,layer_1_size=180,layer_2_size=249,layer_3_size=101,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-06-04/checkpoint_000094)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=410926)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=410926)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=410926)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=410926)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=410574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/checkpoint_000053)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=410926)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_741bb284_93_batch_size=128,layer_1_size=152,layer_2_size=117,layer_3_size=247,layer_4_size=153,layer_5_size=147,layer_2023-10-20_19-07-06/lightning_logs
[2m[36m(RayTrainWorker pid=411463)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=410926)[0m 
[2m[36m(RayTrainWorker pid=410926)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=410926)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=410926)[0m 0 | s1   | Sequential | 156 K 
[2m[36m(RayTrainWorker pid=410926)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=410926)[0m 156 K     Trainable params
[2m[36m(RayTrainWorker pid=410926)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=410926)[0m 156 K     Total params
[2m[36m(RayTrainWorker pid=410926)[0m 0.627     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=410926)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=410926)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=410926)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=410926)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=410926)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=410926)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411164)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=411164)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=411164)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=411164)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=409267)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3fd90e5c_89_batch_size=256,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=79,layer_5_size=32,layer_6__2023-10-20_19-06-07/checkpoint_000096)[32m [repeated 35x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/lightning_logs
[2m[36m(RayTrainWorker pid=411164)[0m 
[2m[36m(RayTrainWorker pid=411164)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=411164)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m 0 | s1   | Sequential | 138 K 
[2m[36m(RayTrainWorker pid=411164)[0m 138 K     Trainable params
[2m[36m(RayTrainWorker pid=411164)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=411164)[0m 138 K     Total params
[2m[36m(RayTrainWorker pid=411164)[0m 0.553     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=411164)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=411164)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=411164)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000741)[32m [repeated 39x across cluster][0m
[2m[36m(RayTrainWorker pid=411463)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=411463)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=411463)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=411463)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=411463)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/lightning_logs
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000252)[32m [repeated 36x across cluster][0m
[2m[36m(RayTrainWorker pid=411463)[0m 
[2m[36m(RayTrainWorker pid=411463)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=411463)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=411463)[0m 0 | s1   | Sequential | 59.3 K
[2m[36m(RayTrainWorker pid=411463)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=411463)[0m 59.3 K    Trainable params
[2m[36m(RayTrainWorker pid=411463)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=411463)[0m 59.3 K    Total params
[2m[36m(RayTrainWorker pid=411463)[0m 0.237     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=411463)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=411463)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411463)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=411463)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411463)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=411463)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=408168)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f702b070_83_batch_size=256,layer_1_size=195,layer_2_size=203,layer_3_size=186,layer_4_size=123,layer_5_size=128,layer_2023-10-20_19-01-47/checkpoint_000147)[32m [repeated 43x across cluster][0m
[2m[36m(TorchTrainer pid=365772)[0m Starting distributed worker processes: ['411774 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000205)[32m [repeated 36x across cluster][0m
[2m[36m(TorchTrainer pid=396257)[0m Starting distributed worker processes: ['411778 (192.0.0.1)'][32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=408437)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc93d21a_84_batch_size=64,layer_1_size=51,layer_2_size=114,layer_3_size=32,layer_4_size=151,layer_5_size=249,layer_6__2023-10-20_19-03-21/checkpoint_000129)[32m [repeated 40x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['412495 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000757)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=411778)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000269)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m Setting up process group for: env:// [rank=0, world_size=1][32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=409955)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/checkpoint_000130)[32m [repeated 36x across cluster][0m
[2m[36m(RayTrainWorker pid=412495)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=411778)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=411778)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=411778)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=411778)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000529)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=411778)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e749e2b3_98_batch_size=256,layer_1_size=248,layer_2_size=249,layer_3_size=99,layer_4_size=100,layer_5_size=114,layer__2023-10-20_19-11-30/lightning_logs
[2m[36m(RayTrainWorker pid=411778)[0m 
[2m[36m(RayTrainWorker pid=411778)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=411778)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=411778)[0m 0 | s1   | Sequential | 196 K 
[2m[36m(RayTrainWorker pid=411778)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=411778)[0m 196 K     Trainable params
[2m[36m(RayTrainWorker pid=411778)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=411778)[0m 196 K     Total params
[2m[36m(RayTrainWorker pid=411778)[0m 0.787     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=411778)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=411778)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411778)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=411778)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411778)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=411778)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411754)[0m GPU available: False, used: False[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m TPU available: False, using: 0 TPU cores[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m IPU available: False, using: 0 IPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m HPU available: False, using: 0 HPUs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411778)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e749e2b3_98_batch_size=256,layer_1_size=248,layer_2_size=249,layer_3_size=99,layer_4_size=100,layer_5_size=114,layer__2023-10-20_19-11-30/checkpoint_000002)[32m [repeated 36x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d41fd2ed_96_batch_size=256,layer_1_size=32,layer_2_size=59,layer_3_size=146,layer_4_size=116,layer_5_size=151,layer_6_2023-10-20_19-10-56/lightning_logs[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m [32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m   | Name | Type       | Params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m ------------------------------------[32m [repeated 4x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m 0 | s1   | Sequential | 87.1 K[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m 87.1 K    Trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m 0         Non-trainable params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m 87.1 K    Total params[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m 0.349     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m   rank_zero_warn([32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=412495)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=412495)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=412495)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=412495)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=408437)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_fc93d21a_84_batch_size=64,layer_1_size=51,layer_2_size=114,layer_3_size=32,layer_4_size=151,layer_5_size=249,layer_6__2023-10-20_19-03-21/checkpoint_000143)[32m [repeated 45x across cluster][0m
[2m[36m(RayTrainWorker pid=412495)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_04c272cb_99_batch_size=32,layer_1_size=207,layer_2_size=121,layer_3_size=121,layer_4_size=147,layer_5_size=155,layer__2023-10-20_19-11-32/lightning_logs
[2m[36m(RayTrainWorker pid=412495)[0m 
[2m[36m(RayTrainWorker pid=412495)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=412495)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=412495)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=412495)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=412495)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=412495)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=412495)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=412495)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=412495)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=412495)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=412495)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=408168)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f702b070_83_batch_size=256,layer_1_size=195,layer_2_size=203,layer_3_size=186,layer_4_size=123,layer_5_size=128,layer_2023-10-20_19-01-47/checkpoint_000171)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=408168)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f702b070_83_batch_size=256,layer_1_size=195,layer_2_size=203,layer_3_size=186,layer_4_size=123,layer_5_size=128,layer_2023-10-20_19-01-47/checkpoint_000174)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000782)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=409955)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/checkpoint_000143)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000038)[32m [repeated 42x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000787)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000542)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000018)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000037)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000233)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=408168)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f702b070_83_batch_size=256,layer_1_size=195,layer_2_size=203,layer_3_size=186,layer_4_size=123,layer_5_size=128,layer_2023-10-20_19-01-47/checkpoint_000192)[32m [repeated 32x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000235)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=412495)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_04c272cb_99_batch_size=32,layer_1_size=207,layer_2_size=121,layer_3_size=121,layer_4_size=147,layer_5_size=155,layer__2023-10-20_19-11-32/checkpoint_000025)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=408168)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_f702b070_83_batch_size=256,layer_1_size=195,layer_2_size=203,layer_3_size=186,layer_4_size=123,layer_5_size=128,layer_2023-10-20_19-01-47/checkpoint_000199)[32m [repeated 38x across cluster][0m
[2m[36m(RayTrainWorker pid=410574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/checkpoint_000116)[32m [repeated 24x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d41fd2ed_96_batch_size=256,layer_1_size=32,layer_2_size=59,layer_3_size=146,layer_4_size=116,layer_5_size=151,layer_6_2023-10-20_19-10-56/checkpoint_000034)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000310)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000049)[32m [repeated 27x across cluster][0m
[2m[36m(TorchTrainer pid=399951)[0m Starting distributed worker processes: ['412956 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000064)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000065)[32m [repeated 24x across cluster][0m
[2m[36m(RayTrainWorker pid=409955)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/checkpoint_000173)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=410574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/checkpoint_000125)[32m [repeated 37x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=411778)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e749e2b3_98_batch_size=256,layer_1_size=248,layer_2_size=249,layer_3_size=99,layer_4_size=100,layer_5_size=114,layer__2023-10-20_19-11-30/checkpoint_000051)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000047)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000818)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000268)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=412956)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=412956)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=412956)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=412956)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2540643b_100_batch_size=256,layer_1_size=159,layer_2_size=249,layer_3_size=32,layer_4_size=249,layer_5_size=209,layer_2023-10-20_19-11-44/lightning_logs
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000270)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m 
[2m[36m(RayTrainWorker pid=412956)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=412956)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=412956)[0m 0 | s1   | Sequential | 55.9 K
[2m[36m(RayTrainWorker pid=412956)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=412956)[0m 55.9 K    Trainable params
[2m[36m(RayTrainWorker pid=412956)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=412956)[0m 55.9 K    Total params
[2m[36m(RayTrainWorker pid=412956)[0m 0.224     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=412956)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=412956)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=412956)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=412956)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=412956)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=412956)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000180)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000576)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=411778)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_e749e2b3_98_batch_size=256,layer_1_size=248,layer_2_size=249,layer_3_size=99,layer_4_size=100,layer_5_size=114,layer__2023-10-20_19-11-30/checkpoint_000065)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000185)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d41fd2ed_96_batch_size=256,layer_1_size=32,layer_2_size=59,layer_3_size=146,layer_4_size=116,layer_5_size=151,layer_6_2023-10-20_19-10-56/checkpoint_000064)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=409955)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/checkpoint_000192)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000065)[32m [repeated 21x across cluster][0m
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['413396 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=412495)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_04c272cb_99_batch_size=32,layer_1_size=207,layer_2_size=121,layer_3_size=121,layer_4_size=147,layer_5_size=155,layer__2023-10-20_19-11-32/checkpoint_000069)[32m [repeated 21x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000334)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2540643b_100_batch_size=256,layer_1_size=159,layer_2_size=249,layer_3_size=32,layer_4_size=249,layer_5_size=209,layer_2023-10-20_19-11-44/checkpoint_000021)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=409955)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/checkpoint_000195)[32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=413396)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=409955)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9716aaf5_90_batch_size=256,layer_1_size=162,layer_2_size=65,layer_3_size=134,layer_4_size=94,layer_5_size=115,layer_6_2023-10-20_19-06-12/checkpoint_000196)[32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000197)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d41fd2ed_96_batch_size=256,layer_1_size=32,layer_2_size=59,layer_3_size=146,layer_4_size=116,layer_5_size=151,layer_6_2023-10-20_19-10-56/checkpoint_000072)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000089)[32m [repeated 31x across cluster][0m
[2m[36m(RayTrainWorker pid=413396)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=413396)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=413396)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=413396)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=413396)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_74b525ca_101_batch_size=32,layer_1_size=246,layer_2_size=249,layer_3_size=195,layer_4_size=249,layer_5_size=249,layer_2023-10-20_19-13-38/lightning_logs
[2m[36m(RayTrainWorker pid=413396)[0m 
[2m[36m(RayTrainWorker pid=413396)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=413396)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=413396)[0m 0 | s1   | Sequential | 156 K 
[2m[36m(RayTrainWorker pid=413396)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=413396)[0m 156 K     Trainable params
[2m[36m(RayTrainWorker pid=413396)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=413396)[0m 156 K     Total params
[2m[36m(RayTrainWorker pid=413396)[0m 0.627     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=413396)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=413396)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=413396)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=413396)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=413396)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=413396)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=410574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/checkpoint_000146)[32m [repeated 35x across cluster][0m
[2m[36m(RayTrainWorker pid=413396)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_74b525ca_101_batch_size=32,layer_1_size=246,layer_2_size=249,layer_3_size=195,layer_4_size=249,layer_5_size=249,layer_2023-10-20_19-13-38/checkpoint_000004)[32m [repeated 31x across cluster][0m
[2m[36m(TorchTrainer pid=383605)[0m Starting distributed worker processes: ['413649 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000206)[32m [repeated 25x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['413842 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000209)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d41fd2ed_96_batch_size=256,layer_1_size=32,layer_2_size=59,layer_3_size=146,layer_4_size=116,layer_5_size=151,layer_6_2023-10-20_19-10-56/checkpoint_000078)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000096)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2540643b_100_batch_size=256,layer_1_size=159,layer_2_size=249,layer_3_size=32,layer_4_size=249,layer_5_size=209,layer_2023-10-20_19-11-44/checkpoint_000039)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=413649)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=412495)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_04c272cb_99_batch_size=32,layer_1_size=207,layer_2_size=121,layer_3_size=121,layer_4_size=147,layer_5_size=155,layer__2023-10-20_19-11-32/checkpoint_000084)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=413842)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=410574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/checkpoint_000151)[32m [repeated 24x across cluster][0m
[2m[36m(TorchTrainer pid=396257)[0m Starting distributed worker processes: ['414348 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000609)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=413649)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=413649)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=413649)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=413649)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=413649)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba03073e_102_batch_size=256,layer_1_size=32,layer_2_size=168,layer_3_size=206,layer_4_size=32,layer_5_size=121,layer__2023-10-20_19-14-59/lightning_logs
[2m[36m(RayTrainWorker pid=413649)[0m 
[2m[36m(RayTrainWorker pid=413649)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=413649)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=413649)[0m 0 | s1   | Sequential | 99.7 K
[2m[36m(RayTrainWorker pid=413649)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=413649)[0m 99.7 K    Trainable params
[2m[36m(RayTrainWorker pid=413649)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=413649)[0m 99.7 K    Total params
[2m[36m(RayTrainWorker pid=413649)[0m 0.399     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=413649)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=413649)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=413649)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=413649)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=413649)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=413649)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000103)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=413842)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=413842)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=413842)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=413842)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=413842)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9ac1188a_103_batch_size=128,layer_1_size=32,layer_2_size=40,layer_3_size=249,layer_4_size=131,layer_5_size=249,layer__2023-10-20_19-15-53/lightning_logs
[2m[36m(RayTrainWorker pid=413842)[0m 
[2m[36m(RayTrainWorker pid=413842)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=413842)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=413842)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=413842)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=413842)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=413842)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=413842)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=413842)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=413842)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=413842)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=413842)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000113)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=414348)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000313)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000128)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=414348)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=414348)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=414348)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=414348)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=414348)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_18e5b6bd_104_batch_size=64,layer_1_size=32,layer_2_size=107,layer_3_size=32,layer_4_size=249,layer_5_size=115,layer_6_2023-10-20_19-16-10/lightning_logs
[2m[36m(RayTrainWorker pid=414348)[0m 
[2m[36m(RayTrainWorker pid=414348)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=414348)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=414348)[0m 0 | s1   | Sequential | 196 K 
[2m[36m(RayTrainWorker pid=414348)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=414348)[0m 196 K     Trainable params
[2m[36m(RayTrainWorker pid=414348)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=414348)[0m 196 K     Total params
[2m[36m(RayTrainWorker pid=414348)[0m 0.787     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=414348)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=414348)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=414348)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=414348)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=414348)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=414348)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000132)[32m [repeated 21x across cluster][0m
[2m[36m(RayTrainWorker pid=413396)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_74b525ca_101_batch_size=32,layer_1_size=246,layer_2_size=249,layer_3_size=195,layer_4_size=249,layer_5_size=249,layer_2023-10-20_19-13-38/checkpoint_000026)[32m [repeated 24x across cluster][0m
[2m[36m(RayTrainWorker pid=411754)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d41fd2ed_96_batch_size=256,layer_1_size=32,layer_2_size=59,layer_3_size=146,layer_4_size=116,layer_5_size=151,layer_6_2023-10-20_19-10-56/checkpoint_000094)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000113)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=413396)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_74b525ca_101_batch_size=32,layer_1_size=246,layer_2_size=249,layer_3_size=195,layer_4_size=249,layer_5_size=249,layer_2023-10-20_19-13-38/checkpoint_000032)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000129)[32m [repeated 32x across cluster][0m
[2m[36m(RayTrainWorker pid=413842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9ac1188a_103_batch_size=128,layer_1_size=32,layer_2_size=40,layer_3_size=249,layer_4_size=131,layer_5_size=249,layer__2023-10-20_19-15-53/checkpoint_000013)[32m [repeated 24x across cluster][0m
[2m[36m(TorchTrainer pid=366455)[0m Starting distributed worker processes: ['414675 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000135)[32m [repeated 33x across cluster][0m
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['414717 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000133)[32m [repeated 31x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000242)[32m [repeated 36x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000142)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000145)[32m [repeated 37x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000386)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=414717)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000894)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000149)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=414675)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=414675)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=414675)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=414675)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/lightning_logs
[2m[36m(RayTrainWorker pid=414675)[0m 
[2m[36m(RayTrainWorker pid=414675)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=414675)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=414675)[0m 0 | s1   | Sequential | 173 K 
[2m[36m(RayTrainWorker pid=414675)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=414675)[0m 173 K     Trainable params
[2m[36m(RayTrainWorker pid=414675)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=414675)[0m 173 K     Total params
[2m[36m(RayTrainWorker pid=414675)[0m 0.694     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=414675)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=414675)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=414717)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=414675)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=413842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9ac1188a_103_batch_size=128,layer_1_size=32,layer_2_size=40,layer_3_size=249,layer_4_size=131,layer_5_size=249,layer__2023-10-20_19-15-53/checkpoint_000037)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=414717)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=414717)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=414717)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=414717)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=414717)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_14a68043_106_batch_size=128,layer_1_size=174,layer_2_size=108,layer_3_size=101,layer_4_size=91,layer_5_size=86,layer__2023-10-20_19-17-35/lightning_logs
[2m[36m(RayTrainWorker pid=414717)[0m 
[2m[36m(RayTrainWorker pid=414717)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=414717)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=414717)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=414717)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=414717)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=414717)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=414717)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=414717)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=414717)[0m   rank_zero_warn([32m [repeated 5x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=414717)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000354)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000154)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000159)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000261)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000395)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000164)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=413842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9ac1188a_103_batch_size=128,layer_1_size=32,layer_2_size=40,layer_3_size=249,layer_4_size=131,layer_5_size=249,layer__2023-10-20_19-15-53/checkpoint_000046)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=413842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9ac1188a_103_batch_size=128,layer_1_size=32,layer_2_size=40,layer_3_size=249,layer_4_size=131,layer_5_size=249,layer__2023-10-20_19-15-53/checkpoint_000047)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000661)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=413396)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_74b525ca_101_batch_size=32,layer_1_size=246,layer_2_size=249,layer_3_size=195,layer_4_size=249,layer_5_size=249,layer_2023-10-20_19-13-38/checkpoint_000079)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=410574)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d4e4231c_92_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=142,layer_4_size=140,layer_5_size=249,layer_6_2023-10-20_19-06-26/checkpoint_000199)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000913)[32m [repeated 11x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2540643b_100_batch_size=256,layer_1_size=159,layer_2_size=249,layer_3_size=32,layer_4_size=249,layer_5_size=209,layer_2023-10-20_19-11-44/checkpoint_000081)[32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2540643b_100_batch_size=256,layer_1_size=159,layer_2_size=249,layer_3_size=32,layer_4_size=249,layer_5_size=209,layer_2023-10-20_19-11-44/checkpoint_000083)[32m [repeated 10x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000409)[32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=414717)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_14a68043_106_batch_size=128,layer_1_size=174,layer_2_size=108,layer_3_size=101,layer_4_size=91,layer_5_size=86,layer__2023-10-20_19-17-35/checkpoint_000018)[32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000412)[32m [repeated 10x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000155)[32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2540643b_100_batch_size=256,layer_1_size=159,layer_2_size=249,layer_3_size=32,layer_4_size=249,layer_5_size=209,layer_2023-10-20_19-11-44/checkpoint_000087)[32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=413396)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_74b525ca_101_batch_size=32,layer_1_size=246,layer_2_size=249,layer_3_size=195,layer_4_size=249,layer_5_size=249,layer_2023-10-20_19-13-38/checkpoint_000088)[32m [repeated 11x across cluster][0m
[2m[36m(TorchTrainer pid=398446)[0m Starting distributed worker processes: ['415219 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000274)[32m [repeated 10x across cluster][0m
[2m[36m(RayTrainWorker pid=413842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9ac1188a_103_batch_size=128,layer_1_size=32,layer_2_size=40,layer_3_size=249,layer_4_size=131,layer_5_size=249,layer__2023-10-20_19-15-53/checkpoint_000056)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000158)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2540643b_100_batch_size=256,layer_1_size=159,layer_2_size=249,layer_3_size=32,layer_4_size=249,layer_5_size=209,layer_2023-10-20_19-11-44/checkpoint_000089)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_000923)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=415219)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=413396)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_74b525ca_101_batch_size=32,layer_1_size=246,layer_2_size=249,layer_3_size=195,layer_4_size=249,layer_5_size=249,layer_2023-10-20_19-13-38/checkpoint_000093)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=413649)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba03073e_102_batch_size=256,layer_1_size=32,layer_2_size=168,layer_3_size=206,layer_4_size=32,layer_5_size=121,layer__2023-10-20_19-14-59/checkpoint_000061)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=413649)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba03073e_102_batch_size=256,layer_1_size=32,layer_2_size=168,layer_3_size=206,layer_4_size=32,layer_5_size=121,layer__2023-10-20_19-14-59/checkpoint_000064)[32m [repeated 31x across cluster][0m
[2m[36m(RayTrainWorker pid=415219)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=415219)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=415219)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=415219)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=415219)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76f436c5_107_batch_size=256,layer_1_size=32,layer_2_size=148,layer_3_size=151,layer_4_size=101,layer_5_size=113,layer_2023-10-20_19-17-47/lightning_logs
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000167)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=415219)[0m 
[2m[36m(RayTrainWorker pid=415219)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=415219)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=415219)[0m 0 | s1   | Sequential | 82.4 K
[2m[36m(RayTrainWorker pid=415219)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=415219)[0m 82.4 K    Trainable params
[2m[36m(RayTrainWorker pid=415219)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=415219)[0m 82.4 K    Total params
[2m[36m(RayTrainWorker pid=415219)[0m 0.330     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=415219)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=415219)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=415219)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=415219)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=415219)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=415219)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000189)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=412956)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_2540643b_100_batch_size=256,layer_1_size=159,layer_2_size=249,layer_3_size=32,layer_4_size=249,layer_5_size=209,layer_2023-10-20_19-11-44/checkpoint_000097)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=411164)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_414f5176_94_batch_size=256,layer_1_size=148,layer_2_size=249,layer_3_size=135,layer_4_size=221,layer_5_size=42,layer__2023-10-20_19-10-20/checkpoint_000189)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=411463)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c00b1152_95_batch_size=32,layer_1_size=118,layer_2_size=127,layer_3_size=249,layer_4_size=249,layer_5_size=249,layer__2023-10-20_19-10-44/checkpoint_000192)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000032)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=415219)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76f436c5_107_batch_size=256,layer_1_size=32,layer_2_size=148,layer_3_size=151,layer_4_size=101,layer_5_size=113,layer_2023-10-20_19-17-47/checkpoint_000006)[32m [repeated 15x across cluster][0m
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['415569 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000034)[32m [repeated 22x across cluster][0m
[2m[36m(TorchTrainer pid=399951)[0m Starting distributed worker processes: ['415611 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000393)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=407614)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c87637f7_81_batch_size=128,layer_1_size=32,layer_2_size=136,layer_3_size=249,layer_4_size=139,layer_5_size=249,layer__2023-10-20_19-00-38/checkpoint_000395)[32m [repeated 25x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000454)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=415569)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=413842)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_9ac1188a_103_batch_size=128,layer_1_size=32,layer_2_size=40,layer_3_size=249,layer_4_size=131,layer_5_size=249,layer__2023-10-20_19-15-53/checkpoint_000079)[32m [repeated 23x across cluster][0m
[2m[36m(TorchTrainer pid=365342)[0m Starting distributed worker processes: ['416049 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=415611)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000185)[32m [repeated 17x across cluster][0m
[2m[36m(TorchTrainer pid=406384)[0m Starting distributed worker processes: ['416046 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000315)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000463)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=415569)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=415569)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=415569)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=415569)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=415569)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d9457640_108_batch_size=32,layer_1_size=144,layer_2_size=249,layer_3_size=143,layer_4_size=148,layer_5_size=249,layer_2023-10-20_19-20-03/lightning_logs
[2m[36m(RayTrainWorker pid=415569)[0m 
[2m[36m(RayTrainWorker pid=415569)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=415569)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=415569)[0m 0 | s1   | Sequential | 156 K 
[2m[36m(RayTrainWorker pid=415569)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=415569)[0m 156 K     Trainable params
[2m[36m(RayTrainWorker pid=415569)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=415569)[0m 156 K     Total params
[2m[36m(RayTrainWorker pid=415569)[0m 0.627     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=415569)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=415569)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=415569)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=415569)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=415569)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=415569)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=415611)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8097837f_109_batch_size=256,layer_1_size=174,layer_2_size=94,layer_3_size=102,layer_4_size=128,layer_5_size=249,layer_2023-10-20_19-21-25/checkpoint_000001)[32m [repeated 40x across cluster][0m
[2m[36m(RayTrainWorker pid=415611)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=415611)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=415611)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=415611)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=416049)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=415611)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8097837f_109_batch_size=256,layer_1_size=174,layer_2_size=94,layer_3_size=102,layer_4_size=128,layer_5_size=249,layer_2023-10-20_19-21-25/lightning_logs
[2m[36m(RayTrainWorker pid=415611)[0m 
[2m[36m(RayTrainWorker pid=415611)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=415611)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=415611)[0m 0 | s1   | Sequential | 55.9 K
[2m[36m(RayTrainWorker pid=415611)[0m 55.9 K    Trainable params
[2m[36m(RayTrainWorker pid=415611)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=415611)[0m 55.9 K    Total params
[2m[36m(RayTrainWorker pid=415611)[0m 0.224     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=415611)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=415611)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=415611)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=415611)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=415569)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d9457640_108_batch_size=32,layer_1_size=144,layer_2_size=249,layer_3_size=143,layer_4_size=148,layer_5_size=249,layer_2023-10-20_19-20-03/checkpoint_000010)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=416049)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=416049)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=416049)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=416049)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=416049)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/lightning_logs
[2m[36m(RayTrainWorker pid=414348)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_18e5b6bd_104_batch_size=64,layer_1_size=32,layer_2_size=107,layer_3_size=32,layer_4_size=249,layer_5_size=115,layer_6_2023-10-20_19-16-10/checkpoint_000080)[32m [repeated 36x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m 
[2m[36m(RayTrainWorker pid=416049)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=416049)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=416049)[0m 0 | s1   | Sequential | 59.3 K
[2m[36m(RayTrainWorker pid=416049)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=416049)[0m 59.3 K    Trainable params
[2m[36m(RayTrainWorker pid=416049)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=416049)[0m 59.3 K    Total params
[2m[36m(RayTrainWorker pid=416049)[0m 0.237     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=416049)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=416049)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416049)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=416049)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416049)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=416049)[0m   rank_zero_warn(
[2m[36m(TorchTrainer pid=407121)[0m Starting distributed worker processes: ['416553 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=416046)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=416046)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=416046)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=416046)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=416046)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/lightning_logs
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000331)[32m [repeated 43x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m 
[2m[36m(RayTrainWorker pid=416046)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=416046)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m 0 | s1   | Sequential | 138 K 
[2m[36m(RayTrainWorker pid=416046)[0m 138 K     Trainable params
[2m[36m(RayTrainWorker pid=416046)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=416046)[0m 138 K     Total params
[2m[36m(RayTrainWorker pid=416046)[0m 0.553     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=416046)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=416046)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=416046)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=415569)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d9457640_108_batch_size=32,layer_1_size=144,layer_2_size=249,layer_3_size=143,layer_4_size=148,layer_5_size=249,layer_2023-10-20_19-20-03/checkpoint_000018)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=416553)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=415611)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8097837f_109_batch_size=256,layer_1_size=174,layer_2_size=94,layer_3_size=102,layer_4_size=128,layer_5_size=249,layer_2023-10-20_19-21-25/checkpoint_000010)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000006)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=416553)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=416553)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=416553)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=416553)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=416553)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0b39e690_112_batch_size=64,layer_1_size=105,layer_2_size=141,layer_3_size=136,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-22-01/lightning_logs
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000474)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=416553)[0m 
[2m[36m(RayTrainWorker pid=416553)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=416553)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=416553)[0m 0 | s1   | Sequential | 227 K 
[2m[36m(RayTrainWorker pid=416553)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=416553)[0m 227 K     Trainable params
[2m[36m(RayTrainWorker pid=416553)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=416553)[0m 227 K     Total params
[2m[36m(RayTrainWorker pid=416553)[0m 0.909     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=416553)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=416553)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416553)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=416553)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416553)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=416553)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000221)[32m [repeated 44x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000225)[32m [repeated 35x across cluster][0m
[2m[36m(TorchTrainer pid=367888)[0m Starting distributed worker processes: ['416968 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=415569)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d9457640_108_batch_size=32,layer_1_size=144,layer_2_size=249,layer_3_size=143,layer_4_size=148,layer_5_size=249,layer_2023-10-20_19-20-03/checkpoint_000031)[32m [repeated 39x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/checkpoint_000017)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000015)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000730)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=416968)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=415569)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d9457640_108_batch_size=32,layer_1_size=144,layer_2_size=249,layer_3_size=143,layer_4_size=148,layer_5_size=249,layer_2023-10-20_19-20-03/checkpoint_000036)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000736)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=413649)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba03073e_102_batch_size=256,layer_1_size=32,layer_2_size=168,layer_3_size=206,layer_4_size=32,layer_5_size=121,layer__2023-10-20_19-14-59/checkpoint_000127)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=416968)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=416968)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=416968)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=416968)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=416968)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_82dfe4fa_113_batch_size=32,layer_1_size=32,layer_2_size=135,layer_3_size=249,layer_4_size=132,layer_5_size=249,layer__2023-10-20_19-22-25/lightning_logs
[2m[36m(TorchTrainer pid=396257)[0m Starting distributed worker processes: ['417246 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=416968)[0m 
[2m[36m(RayTrainWorker pid=416968)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=416968)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=416968)[0m 0 | s1   | Sequential | 113 K 
[2m[36m(RayTrainWorker pid=416968)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=416968)[0m 113 K     Trainable params
[2m[36m(RayTrainWorker pid=416968)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=416968)[0m 113 K     Total params
[2m[36m(RayTrainWorker pid=416968)[0m 0.456     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=416968)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=416968)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416968)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=416968)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416968)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=416968)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416553)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0b39e690_112_batch_size=64,layer_1_size=105,layer_2_size=141,layer_3_size=136,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-22-01/checkpoint_000024)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000084)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/checkpoint_000027)[32m [repeated 21x across cluster][0m
[2m[36m(RayTrainWorker pid=417246)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000746)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000245)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=417246)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=417246)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=417246)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=417246)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=417246)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_487747f1_114_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=100,layer_4_size=180,layer_5_size=115,layer_6_2023-10-20_19-23-03/lightning_logs
[2m[36m(RayTrainWorker pid=417246)[0m 
[2m[36m(RayTrainWorker pid=417246)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=417246)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=417246)[0m 0 | s1   | Sequential | 196 K 
[2m[36m(RayTrainWorker pid=417246)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=417246)[0m 196 K     Trainable params
[2m[36m(RayTrainWorker pid=417246)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=417246)[0m 196 K     Total params
[2m[36m(RayTrainWorker pid=417246)[0m 0.787     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=417246)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=417246)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=417246)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=417246)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=417246)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=417246)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416553)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0b39e690_112_batch_size=64,layer_1_size=105,layer_2_size=141,layer_3_size=136,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-22-01/checkpoint_000032)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000750)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000091)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=415219)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76f436c5_107_batch_size=256,layer_1_size=32,layer_2_size=148,layer_3_size=151,layer_4_size=101,layer_5_size=113,layer_2023-10-20_19-17-47/checkpoint_000068)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/checkpoint_000032)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000095)[32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=417246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_487747f1_114_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=100,layer_4_size=180,layer_5_size=115,layer_6_2023-10-20_19-23-03/checkpoint_000006)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_001002)[32m [repeated 7x across cluster][0m
[2m[36m(RayTrainWorker pid=415219)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76f436c5_107_batch_size=256,layer_1_size=32,layer_2_size=148,layer_3_size=151,layer_4_size=101,layer_5_size=113,layer_2023-10-20_19-17-47/checkpoint_000071)[32m [repeated 11x across cluster][0m
[2m[36m(RayTrainWorker pid=416968)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_82dfe4fa_113_batch_size=32,layer_1_size=32,layer_2_size=135,layer_3_size=249,layer_4_size=132,layer_5_size=249,layer__2023-10-20_19-22-25/checkpoint_000013)[32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=413649)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba03073e_102_batch_size=256,layer_1_size=32,layer_2_size=168,layer_3_size=206,layer_4_size=32,layer_5_size=121,layer__2023-10-20_19-14-59/checkpoint_000149)[32m [repeated 12x across cluster][0m
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000377)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000033)[32m [repeated 13x across cluster][0m
[2m[36m(RayTrainWorker pid=416553)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0b39e690_112_batch_size=64,layer_1_size=105,layer_2_size=141,layer_3_size=136,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-22-01/checkpoint_000042)[32m [repeated 10x across cluster][0m
[2m[36m(RayTrainWorker pid=414717)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_14a68043_106_batch_size=128,layer_1_size=174,layer_2_size=108,layer_3_size=101,layer_4_size=91,layer_5_size=86,layer__2023-10-20_19-17-35/checkpoint_000089)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000097)[32m [repeated 19x across cluster][0m
[2m[36m(RayTrainWorker pid=414717)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_14a68043_106_batch_size=128,layer_1_size=174,layer_2_size=108,layer_3_size=101,layer_4_size=91,layer_5_size=86,layer__2023-10-20_19-17-35/checkpoint_000095)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000760)[32m [repeated 23x across cluster][0m
[2m[36m(RayTrainWorker pid=415219)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76f436c5_107_batch_size=256,layer_1_size=32,layer_2_size=148,layer_3_size=151,layer_4_size=101,layer_5_size=113,layer_2023-10-20_19-17-47/checkpoint_000083)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000763)[32m [repeated 21x across cluster][0m
[2m[36m(RayTrainWorker pid=416553)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0b39e690_112_batch_size=64,layer_1_size=105,layer_2_size=141,layer_3_size=136,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-22-01/checkpoint_000051)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000526)[32m [repeated 26x across cluster][0m
[2m[36m(RayTrainWorker pid=417246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_487747f1_114_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=100,layer_4_size=180,layer_5_size=115,layer_6_2023-10-20_19-23-03/checkpoint_000024)[32m [repeated 24x across cluster][0m
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000106)[32m [repeated 25x across cluster][0m
[2m[36m(TorchTrainer pid=371531)[0m Starting distributed worker processes: ['417747 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=417246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_487747f1_114_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=100,layer_4_size=180,layer_5_size=115,layer_6_2023-10-20_19-23-03/checkpoint_000027)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=415219)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_76f436c5_107_batch_size=256,layer_1_size=32,layer_2_size=148,layer_3_size=151,layer_4_size=101,layer_5_size=113,layer_2023-10-20_19-17-47/checkpoint_000096)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_001023)[32m [repeated 32x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/checkpoint_000056)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=417747)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=416046)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/checkpoint_000058)[32m [repeated 35x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000295)[32m [repeated 23x across cluster][0m
[2m[36m(TorchTrainer pid=398446)[0m Starting distributed worker processes: ['417997 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=417246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_487747f1_114_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=100,layer_4_size=180,layer_5_size=115,layer_6_2023-10-20_19-23-03/checkpoint_000038)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=417747)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=417747)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=417747)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=417747)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=417747)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_1159c626_115_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=182,layer_4_size=171,layer_5_size=67,layer_6__2023-10-20_19-23-41/lightning_logs
[2m[36m(RayTrainWorker pid=409962)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_40c0b100_91_batch_size=256,layer_1_size=173,layer_2_size=249,layer_3_size=132,layer_4_size=114,layer_5_size=249,layer_2023-10-20_19-06-23/checkpoint_000398)[32m [repeated 14x across cluster][0m
[2m[36m(RayTrainWorker pid=417747)[0m 
[2m[36m(RayTrainWorker pid=417747)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=417747)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=417747)[0m 0 | s1   | Sequential | 87.1 K
[2m[36m(RayTrainWorker pid=417747)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=417747)[0m 87.1 K    Trainable params
[2m[36m(RayTrainWorker pid=417747)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=417747)[0m 87.1 K    Total params
[2m[36m(RayTrainWorker pid=417747)[0m 0.349     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=417747)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=417747)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=417747)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=417747)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=417747)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=417747)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=415611)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8097837f_109_batch_size=256,layer_1_size=174,layer_2_size=94,layer_3_size=102,layer_4_size=128,layer_5_size=249,layer_2023-10-20_19-21-25/checkpoint_000069)[32m [repeated 22x across cluster][0m
[2m[36m(RayTrainWorker pid=417997)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=415569)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d9457640_108_batch_size=32,layer_1_size=144,layer_2_size=249,layer_3_size=143,layer_4_size=148,layer_5_size=249,layer_2023-10-20_19-20-03/checkpoint_000080)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=417246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_487747f1_114_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=100,layer_4_size=180,layer_5_size=115,layer_6_2023-10-20_19-23-03/checkpoint_000046)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000072)[32m [repeated 27x across cluster][0m
[2m[36m(RayTrainWorker pid=417997)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=417997)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=417997)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=417997)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=417997)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b080c670_116_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_19-26-05/lightning_logs
[2m[36m(RayTrainWorker pid=413649)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba03073e_102_batch_size=256,layer_1_size=32,layer_2_size=168,layer_3_size=206,layer_4_size=32,layer_5_size=121,layer__2023-10-20_19-14-59/checkpoint_000182)[32m [repeated 21x across cluster][0m
[2m[36m(RayTrainWorker pid=417997)[0m 
[2m[36m(RayTrainWorker pid=417997)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=417997)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=417997)[0m 0 | s1   | Sequential | 82.4 K
[2m[36m(RayTrainWorker pid=417997)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=417997)[0m 82.4 K    Trainable params
[2m[36m(RayTrainWorker pid=417997)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=417997)[0m 82.4 K    Total params
[2m[36m(RayTrainWorker pid=417997)[0m 0.330     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=417997)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=417997)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=417997)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=417997)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=417997)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=417997)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000550)[32m [repeated 28x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_001039)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=417246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_487747f1_114_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=100,layer_4_size=180,layer_5_size=115,layer_6_2023-10-20_19-23-03/checkpoint_000056)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=416553)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0b39e690_112_batch_size=64,layer_1_size=105,layer_2_size=141,layer_3_size=136,layer_4_size=32,layer_5_size=249,layer__2023-10-20_19-22-01/checkpoint_000085)[32m [repeated 33x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/checkpoint_000081)[32m [repeated 35x across cluster][0m
[2m[36m(RayTrainWorker pid=415569)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_d9457640_108_batch_size=32,layer_1_size=144,layer_2_size=249,layer_3_size=143,layer_4_size=148,layer_5_size=249,layer_2023-10-20_19-20-03/checkpoint_000093)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000319)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000088)[32m [repeated 38x across cluster][0m
[2m[36m(RayTrainWorker pid=413649)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_ba03073e_102_batch_size=256,layer_1_size=32,layer_2_size=168,layer_3_size=206,layer_4_size=32,layer_5_size=121,layer__2023-10-20_19-14-59/checkpoint_000197)[32m [repeated 34x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/checkpoint_000093)[32m [repeated 30x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000094)[32m [repeated 29x across cluster][0m
[2m[36m(RayTrainWorker pid=393000)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_eecd7a23_52_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=76,layer_5_size=192,layer_6__2023-10-20_18-49-13/checkpoint_001055)[32m [repeated 30x across cluster][0m
[2m[36m(TorchTrainer pid=389993)[0m Starting distributed worker processes: ['418290 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000144)[32m [repeated 11x across cluster][0m
[2m[36m(TorchTrainer pid=407121)[0m Starting distributed worker processes: ['418331 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000145)[32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=417997)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b080c670_116_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_19-26-05/checkpoint_000022)[32m [repeated 7x across cluster][0m
[2m[36m(RayTrainWorker pid=417747)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_1159c626_115_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=182,layer_4_size=171,layer_5_size=67,layer_6__2023-10-20_19-23-41/checkpoint_000032)[32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=416046)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8d1b9a52_110_batch_size=128,layer_1_size=224,layer_2_size=95,layer_3_size=249,layer_4_size=55,layer_5_size=46,layer_6_2023-10-20_19-21-36/checkpoint_000104)[32m [repeated 9x across cluster][0m
[2m[36m(RayTrainWorker pid=418331)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000581)[32m [repeated 7x across cluster][0m
[2m[36m(RayTrainWorker pid=418290)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(TorchTrainer pid=383605)[0m Starting distributed worker processes: ['418838 (192.0.0.1)']
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000583)[32m [repeated 10x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000103)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=418331)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=418331)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=418331)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=418331)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=418331)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_0faf860b_118_batch_size=32,layer_1_size=247,layer_2_size=249,layer_3_size=90,layer_4_size=140,layer_5_size=249,layer__2023-10-20_19-28-15/lightning_logs
[2m[36m(RayTrainWorker pid=418331)[0m 
[2m[36m(RayTrainWorker pid=418331)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=418331)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=418331)[0m 0 | s1   | Sequential | 227 K 
[2m[36m(RayTrainWorker pid=418331)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=418331)[0m 227 K     Trainable params
[2m[36m(RayTrainWorker pid=418331)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=418331)[0m 227 K     Total params
[2m[36m(RayTrainWorker pid=418331)[0m 0.909     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=418331)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=418331)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=418331)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=418331)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=418331)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=418331)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=416968)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_82dfe4fa_113_batch_size=32,layer_1_size=32,layer_2_size=135,layer_3_size=249,layer_4_size=132,layer_5_size=249,layer__2023-10-20_19-22-25/checkpoint_000079)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=418838)[0m Setting up process group for: env:// [rank=0, world_size=1]
[2m[36m(RayTrainWorker pid=418290)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=418290)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=418290)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=418290)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=418290)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3786a26e_117_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=119,layer_4_size=141,layer_5_size=249,layer__2023-10-20_19-26-45/lightning_logs
[2m[36m(RayTrainWorker pid=418290)[0m 
[2m[36m(RayTrainWorker pid=418290)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=418290)[0m ------------------------------------[32m [repeated 2x across cluster][0m
[2m[36m(RayTrainWorker pid=418290)[0m 0 | s1   | Sequential | 156 K 
[2m[36m(RayTrainWorker pid=418290)[0m 156 K     Trainable params
[2m[36m(RayTrainWorker pid=418290)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=418290)[0m 156 K     Total params
[2m[36m(RayTrainWorker pid=418290)[0m 0.627     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=418290)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=418290)[0m   rank_zero_warn([32m [repeated 3x across cluster][0m
[2m[36m(RayTrainWorker pid=418290)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=418290)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=417747)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_1159c626_115_batch_size=32,layer_1_size=32,layer_2_size=72,layer_3_size=182,layer_4_size=171,layer_5_size=67,layer_6__2023-10-20_19-23-41/checkpoint_000039)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=418838)[0m GPU available: False, used: False
[2m[36m(RayTrainWorker pid=418838)[0m TPU available: False, using: 0 TPU cores
[2m[36m(RayTrainWorker pid=418838)[0m IPU available: False, using: 0 IPUs
[2m[36m(RayTrainWorker pid=418838)[0m HPU available: False, using: 0 HPUs
[2m[36m(RayTrainWorker pid=418838)[0m Missing logger folder: /rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_60bf9055_119_batch_size=128,layer_1_size=168,layer_2_size=249,layer_3_size=198,layer_4_size=128,layer_5_size=249,laye_2023-10-20_19-28-20/lightning_logs
[2m[36m(RayTrainWorker pid=417997)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b080c670_116_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_19-26-05/checkpoint_000027)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=418838)[0m 
[2m[36m(RayTrainWorker pid=418838)[0m   | Name | Type       | Params
[2m[36m(RayTrainWorker pid=418838)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=418838)[0m 0 | s1   | Sequential | 99.7 K
[2m[36m(RayTrainWorker pid=418838)[0m ------------------------------------
[2m[36m(RayTrainWorker pid=418838)[0m 99.7 K    Trainable params
[2m[36m(RayTrainWorker pid=418838)[0m 0         Non-trainable params
[2m[36m(RayTrainWorker pid=418838)[0m 99.7 K    Total params
[2m[36m(RayTrainWorker pid=418838)[0m 0.399     Total estimated model params size (MB)
[2m[36m(RayTrainWorker pid=418838)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=418838)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=418838)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(RayTrainWorker pid=418838)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=418838)[0m /rds/general/user/dcm120/home/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[2m[36m(RayTrainWorker pid=418838)[0m   rank_zero_warn(
[2m[36m(RayTrainWorker pid=414675)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b20470f6_105_batch_size=32,layer_1_size=117,layer_2_size=249,layer_3_size=143,layer_4_size=144,layer_5_size=141,layer_2023-10-20_19-16-33/checkpoint_000152)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=418290)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_3786a26e_117_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=119,layer_4_size=141,layer_5_size=249,layer__2023-10-20_19-26-45/checkpoint_000004)[32m [repeated 15x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000108)[32m [repeated 18x across cluster][0m
[2m[36m(RayTrainWorker pid=417997)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b080c670_116_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_19-26-05/checkpoint_000032)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=417997)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b080c670_116_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_19-26-05/checkpoint_000033)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=415611)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8097837f_109_batch_size=256,layer_1_size=174,layer_2_size=94,layer_3_size=102,layer_4_size=128,layer_5_size=249,layer_2023-10-20_19-21-25/checkpoint_000120)[32m [repeated 20x across cluster][0m
[2m[36m(RayTrainWorker pid=415611)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8097837f_109_batch_size=256,layer_1_size=174,layer_2_size=94,layer_3_size=102,layer_4_size=128,layer_5_size=249,layer_2023-10-20_19-21-25/checkpoint_000121)[32m [repeated 8x across cluster][0m
[2m[36m(RayTrainWorker pid=415611)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_8097837f_109_batch_size=256,layer_1_size=174,layer_2_size=94,layer_3_size=102,layer_4_size=128,layer_5_size=249,layer_2023-10-20_19-21-25/checkpoint_000122)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=396256)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_c50fab98_57_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=115,layer_4_size=32,layer_5_size=241,layer_6__2023-10-20_18-50-34/checkpoint_000841)[32m [repeated 7x across cluster][0m
[2m[36m(RayTrainWorker pid=418838)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_60bf9055_119_batch_size=128,layer_1_size=168,layer_2_size=249,layer_3_size=198,layer_4_size=128,layer_5_size=249,laye_2023-10-20_19-28-20/checkpoint_000006)[32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000115)[32m [repeated 6x across cluster][0m
[2m[36m(RayTrainWorker pid=403460)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_32a3b4b4_71_batch_size=32,layer_1_size=152,layer_2_size=249,layer_3_size=135,layer_4_size=127,layer_5_size=249,layer__2023-10-20_18-57-49/checkpoint_000600)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=416049)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_86bc3e60_111_batch_size=32,layer_1_size=32,layer_2_size=111,layer_3_size=249,layer_4_size=133,layer_5_size=73,layer_6_2023-10-20_19-21-58/checkpoint_000117)[32m [repeated 17x across cluster][0m
[2m[36m(RayTrainWorker pid=411774)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_4e0c6b0a_97_batch_size=128,layer_1_size=159,layer_2_size=94,layer_3_size=148,layer_4_size=121,layer_5_size=249,layer__2023-10-20_19-11-25/checkpoint_000360)[32m [repeated 16x across cluster][0m
[2m[36m(RayTrainWorker pid=417997)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/rds/general/user/dcm120/home/ray_results/TorchTrainer_2023-10-20_18-32-35/TorchTrainer_b080c670_116_batch_size=32,layer_1_size=32,layer_2_size=249,layer_3_size=249,layer_4_size=32,layer_5_size=249,layer_6_2023-10-20_19-26-05/checkpoint_000041)[32m [repeated 30x across cluster][0m
=>> PBS: job killed: walltime 3610 exceeded limit 3600
